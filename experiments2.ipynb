{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses an embedding representation of the words in the vocabulary which is then passed through a BiLSTM to predict the NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a word index and returns word\n",
    "def i2w(index):\n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a tag index and returns tag\n",
    "def i2t(index):\n",
    "     return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag]))\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list\n",
    "\n",
    "# [(1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 1), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    '''\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    '''\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    '''\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    '''\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=7.5156, time=8.37s\n",
      "iter 0: test acc=0.9074\n",
      "iter 1: train loss/sent=5.2781, time=8.24s\n",
      "iter 1: test acc=0.9098\n",
      "iter 2: train loss/sent=4.2173, time=8.49s\n",
      "iter 2: test acc=0.9097\n",
      "iter 3: train loss/sent=3.4142, time=8.47s\n",
      "iter 3: test acc=0.9040\n",
      "iter 4: train loss/sent=2.7021, time=8.30s\n",
      "iter 4: test acc=0.8822\n",
      "iter 5: train loss/sent=2.1130, time=8.32s\n",
      "iter 5: test acc=0.8814\n",
      "iter 6: train loss/sent=1.5663, time=8.26s\n",
      "iter 6: test acc=0.8674\n",
      "iter 7: train loss/sent=1.1498, time=8.29s\n",
      "iter 7: test acc=0.8549\n",
      "iter 8: train loss/sent=0.7723, time=8.28s\n",
      "iter 8: test acc=0.8688\n",
      "iter 9: train loss/sent=0.4873, time=8.33s\n",
      "iter 9: test acc=0.8754\n",
      "iter 10: train loss/sent=0.3345, time=8.36s\n",
      "iter 10: test acc=0.8574\n",
      "iter 11: train loss/sent=0.2257, time=8.27s\n",
      "iter 11: test acc=0.8764\n",
      "iter 12: train loss/sent=0.1604, time=8.31s\n",
      "iter 12: test acc=0.8803\n",
      "iter 13: train loss/sent=0.1199, time=8.35s\n",
      "iter 13: test acc=0.8729\n",
      "iter 14: train loss/sent=0.1060, time=8.38s\n",
      "iter 14: test acc=0.8689\n",
      "iter 15: train loss/sent=0.0917, time=8.28s\n",
      "iter 15: test acc=0.8764\n",
      "iter 16: train loss/sent=0.0808, time=8.23s\n",
      "iter 16: test acc=0.8756\n",
      "iter 17: train loss/sent=0.0871, time=8.33s\n",
      "iter 17: test acc=0.8939\n",
      "iter 18: train loss/sent=0.0743, time=8.29s\n",
      "iter 18: test acc=0.8970\n",
      "iter 19: train loss/sent=0.0847, time=8.43s\n",
      "iter 19: test acc=0.8905\n",
      "iter 20: train loss/sent=0.0657, time=8.38s\n",
      "iter 20: test acc=0.9041\n",
      "iter 21: train loss/sent=0.0666, time=8.33s\n",
      "iter 21: test acc=0.8981\n",
      "iter 22: train loss/sent=0.0714, time=8.30s\n",
      "iter 22: test acc=0.8908\n",
      "iter 23: train loss/sent=0.0687, time=8.25s\n",
      "iter 23: test acc=0.8945\n",
      "iter 24: train loss/sent=0.0687, time=8.33s\n",
      "iter 24: test acc=0.8988\n",
      "iter 25: train loss/sent=0.0565, time=8.25s\n",
      "iter 25: test acc=0.9018\n",
      "iter 26: train loss/sent=0.0654, time=8.30s\n",
      "iter 26: test acc=0.9066\n",
      "iter 27: train loss/sent=0.0579, time=8.37s\n",
      "iter 27: test acc=0.8954\n",
      "iter 28: train loss/sent=0.0606, time=8.31s\n",
      "iter 28: test acc=0.9005\n",
      "iter 29: train loss/sent=0.0546, time=8.29s\n",
      "iter 29: test acc=0.8984\n",
      "iter 30: train loss/sent=0.0634, time=8.27s\n",
      "iter 30: test acc=0.8899\n",
      "iter 31: train loss/sent=0.0520, time=8.27s\n",
      "iter 31: test acc=0.9016\n",
      "iter 32: train loss/sent=0.0509, time=8.32s\n",
      "iter 32: test acc=0.9027\n",
      "iter 33: train loss/sent=0.0491, time=8.30s\n",
      "iter 33: test acc=0.8999\n",
      "iter 34: train loss/sent=0.0444, time=8.32s\n",
      "iter 34: test acc=0.9020\n",
      "iter 35: train loss/sent=0.0476, time=8.33s\n",
      "iter 35: test acc=0.8945\n",
      "iter 36: train loss/sent=0.0467, time=8.26s\n",
      "iter 36: test acc=0.8927\n",
      "iter 37: train loss/sent=0.0435, time=8.26s\n",
      "iter 37: test acc=0.9103\n",
      "iter 38: train loss/sent=0.0488, time=8.30s\n",
      "iter 38: test acc=0.8878\n",
      "iter 39: train loss/sent=0.0379, time=8.29s\n",
      "iter 39: test acc=0.9036\n",
      "iter 40: train loss/sent=0.0441, time=8.31s\n",
      "iter 40: test acc=0.9058\n",
      "iter 41: train loss/sent=0.0412, time=8.34s\n",
      "iter 41: test acc=0.8988\n",
      "iter 42: train loss/sent=0.0410, time=8.32s\n",
      "iter 42: test acc=0.9097\n",
      "iter 43: train loss/sent=0.0341, time=8.27s\n",
      "iter 43: test acc=0.8879\n",
      "iter 44: train loss/sent=0.0398, time=8.44s\n",
      "iter 44: test acc=0.9054\n",
      "iter 45: train loss/sent=0.0408, time=8.32s\n",
      "iter 45: test acc=0.9025\n",
      "iter 46: train loss/sent=0.0377, time=8.22s\n",
      "iter 46: test acc=0.8948\n",
      "iter 47: train loss/sent=0.0345, time=8.35s\n",
      "iter 47: test acc=0.8917\n",
      "iter 48: train loss/sent=0.0357, time=8.44s\n",
      "iter 48: test acc=0.9079\n",
      "iter 49: train loss/sent=0.0362, time=8.35s\n",
      "iter 49: test acc=0.8979\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(50):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == 49:\n",
    "        out = open('predicted1.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == 49:\n",
    "                out.write(i2w(word) + ' ' + i2t(gold) + ' ' + i2t(predicted) + '\\n')\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "    if iter == 49:\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the 200 dimension pre-trained Glove embeddings to represent the words which are then passed through the BiLSTM to predict the NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"unk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a word index and returns word\n",
    "def i2w(index):\n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a tag index and returns tag\n",
    "def i2t(index):\n",
    "     return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((word, t2i[tag]))\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list\n",
    "\n",
    "# [(1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 1), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = read_dataset(\"wnut17/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load the pre-trained Glove embeddings\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "vectors = []\n",
    "vectors.append(list(np.zeros(200))) # 200 zeros or random?\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.200d.txt\") as f:\n",
    "    f.readline()\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        fields = line.strip().split(\" \")\n",
    "        vocab[fields[0]]\n",
    "        vectors.append(list(map(float, fields[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "lookup = model.add_lookup_parameters((len(vectors), len(vectors[0])))\n",
    "lookup.init_from_array(np.array(vectors))\n",
    "print(lookup[vocab[\"hello\"]].value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word, update=False) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append(predict)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ITER in range(50):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    total_acc = 0.0\n",
    "    for sent in dev:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for predicted, gold in zip(p_labels, g_labels):\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(dev)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
