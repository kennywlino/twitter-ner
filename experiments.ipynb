{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses an embedding representation of the words in the vocabulary which is then passed through a BiLSTM to predict the NER labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 labels model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=7.4537, time=8.34s\n",
      "iter 0: test acc=0.9074\n",
      "iter 1: train loss/sent=5.3288, time=7.71s\n",
      "iter 1: test acc=0.9085\n",
      "iter 2: train loss/sent=4.2602, time=8.04s\n",
      "iter 2: test acc=0.9113\n",
      "iter 3: train loss/sent=3.4010, time=8.43s\n",
      "iter 3: test acc=0.9111\n",
      "iter 4: train loss/sent=2.6634, time=7.99s\n",
      "iter 4: test acc=0.9122\n",
      "iter 5: train loss/sent=2.0325, time=8.69s\n",
      "iter 5: test acc=0.9126\n",
      "iter 6: train loss/sent=1.4901, time=8.86s\n",
      "iter 6: test acc=0.9123\n",
      "iter 7: train loss/sent=1.0708, time=8.37s\n",
      "iter 7: test acc=0.9130\n",
      "iter 8: train loss/sent=0.7558, time=8.27s\n",
      "iter 8: test acc=0.9131\n",
      "iter 9: train loss/sent=0.4685, time=8.61s\n",
      "iter 9: test acc=0.9125\n",
      "iter 10: train loss/sent=0.3234, time=8.53s\n",
      "iter 10: test acc=0.9127\n",
      "iter 11: train loss/sent=0.2138, time=8.52s\n",
      "iter 11: test acc=0.9129\n",
      "iter 12: train loss/sent=0.1567, time=8.28s\n",
      "iter 12: test acc=0.9130\n",
      "iter 13: train loss/sent=0.1143, time=8.58s\n",
      "iter 13: test acc=0.9136\n",
      "iter 14: train loss/sent=0.1108, time=8.30s\n",
      "iter 14: test acc=0.9144\n",
      "iter 15: train loss/sent=0.1055, time=8.46s\n",
      "iter 15: test acc=0.9138\n",
      "iter 16: train loss/sent=0.1018, time=8.25s\n",
      "iter 16: test acc=0.9143\n",
      "iter 17: train loss/sent=0.0786, time=8.39s\n",
      "iter 17: test acc=0.9141\n",
      "iter 18: train loss/sent=0.0800, time=8.59s\n",
      "iter 18: test acc=0.9148\n",
      "iter 19: train loss/sent=0.0719, time=8.21s\n",
      "iter 19: test acc=0.9149\n",
      "iter 20: train loss/sent=0.0739, time=8.05s\n",
      "iter 20: test acc=0.9143\n",
      "iter 21: train loss/sent=0.0741, time=8.30s\n",
      "iter 21: test acc=0.9150\n",
      "iter 22: train loss/sent=0.0683, time=8.22s\n",
      "iter 22: test acc=0.9152\n",
      "iter 23: train loss/sent=0.0670, time=8.24s\n",
      "iter 23: test acc=0.9144\n",
      "iter 24: train loss/sent=0.0707, time=8.60s\n",
      "iter 24: test acc=0.9152\n",
      "iter 25: train loss/sent=0.0777, time=8.51s\n",
      "iter 25: test acc=0.9150\n",
      "iter 26: train loss/sent=0.0684, time=8.60s\n",
      "iter 26: test acc=0.9156\n",
      "iter 27: train loss/sent=0.0604, time=8.20s\n",
      "iter 27: test acc=0.9153\n",
      "iter 28: train loss/sent=0.0611, time=8.29s\n",
      "iter 28: test acc=0.9148\n",
      "iter 29: train loss/sent=0.0627, time=8.46s\n",
      "iter 29: test acc=0.9153\n",
      "iter 30: train loss/sent=0.0618, time=8.47s\n",
      "iter 30: test acc=0.9158\n",
      "iter 31: train loss/sent=0.0572, time=8.43s\n",
      "iter 31: test acc=0.9155\n",
      "iter 32: train loss/sent=0.0579, time=8.28s\n",
      "iter 32: test acc=0.9152\n",
      "iter 33: train loss/sent=0.0533, time=8.18s\n",
      "iter 33: test acc=0.9151\n",
      "iter 34: train loss/sent=0.0533, time=8.10s\n",
      "iter 34: test acc=0.9153\n",
      "iter 35: train loss/sent=0.0469, time=8.28s\n",
      "iter 35: test acc=0.9152\n",
      "iter 36: train loss/sent=0.0448, time=8.06s\n",
      "iter 36: test acc=0.9161\n",
      "iter 37: train loss/sent=0.0465, time=7.93s\n",
      "iter 37: test acc=0.9159\n",
      "iter 38: train loss/sent=0.0452, time=7.92s\n",
      "iter 38: test acc=0.9158\n",
      "iter 39: train loss/sent=0.0432, time=7.93s\n",
      "iter 39: test acc=0.9155\n",
      "iter 40: train loss/sent=0.0426, time=7.92s\n",
      "iter 40: test acc=0.9154\n",
      "iter 41: train loss/sent=0.0424, time=7.93s\n",
      "iter 41: test acc=0.9157\n",
      "iter 42: train loss/sent=0.0383, time=7.94s\n",
      "iter 42: test acc=0.9156\n",
      "iter 43: train loss/sent=0.0440, time=8.06s\n",
      "iter 43: test acc=0.9152\n",
      "iter 44: train loss/sent=0.0391, time=8.15s\n",
      "iter 44: test acc=0.9155\n",
      "iter 45: train loss/sent=0.0374, time=8.11s\n",
      "iter 45: test acc=0.9162\n",
      "iter 46: train loss/sent=0.0366, time=8.10s\n",
      "iter 46: test acc=0.9160\n",
      "iter 47: train loss/sent=0.0342, time=8.11s\n",
      "iter 47: test acc=0.9163\n",
      "iter 48: train loss/sent=0.0356, time=8.09s\n",
      "iter 48: test acc=0.9155\n",
      "iter 49: train loss/sent=0.0357, time=8.09s\n",
      "iter 49: test acc=0.9155\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-1.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 61908 tokens with 3621 phrases; found: 1471 phrases; correct: 497.\n",
      "accuracy:  91.24%; precision:  33.79%; recall:  13.73%; FB1:  19.52\n",
      "          company: precision:  53.66%; recall:  10.33%; FB1:  17.32  123\n",
      "         facility: precision:   7.21%; recall:   3.01%; FB1:   4.24  111\n",
      "          geo-loc: precision:  51.20%; recall:  28.86%; FB1:  36.91  500\n",
      "            movie: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "      musicartist: precision:  30.30%; recall:   5.10%; FB1:   8.73  33\n",
      "            other: precision:  23.99%; recall:  14.26%; FB1:  17.89  371\n",
      "           person: precision:  21.98%; recall:  10.16%; FB1:  13.90  232\n",
      "          product: precision:  25.81%; recall:   5.63%; FB1:   9.25  62\n",
      "       sportsteam: precision:   3.23%; recall:   0.66%; FB1:   1.10  31\n",
      "           tvshow: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n"
     ]
    }
   ],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 labels model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train_notypes\")\n",
    "dev = read_dataset(\"wnut17/data/dev_notypes\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test_notypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=4.5196, time=9.37s\n",
      "iter 0: test acc=0.9084\n",
      "iter 1: train loss/sent=2.9940, time=8.65s\n",
      "iter 1: test acc=0.9136\n",
      "iter 2: train loss/sent=2.2171, time=8.09s\n",
      "iter 2: test acc=0.9187\n",
      "iter 3: train loss/sent=1.5927, time=7.80s\n",
      "iter 3: test acc=0.9162\n",
      "iter 4: train loss/sent=1.0688, time=7.77s\n",
      "iter 4: test acc=0.9167\n",
      "iter 5: train loss/sent=0.6863, time=7.91s\n",
      "iter 5: test acc=0.9117\n",
      "iter 6: train loss/sent=0.4192, time=8.08s\n",
      "iter 6: test acc=0.9127\n",
      "iter 7: train loss/sent=0.2398, time=7.74s\n",
      "iter 7: test acc=0.9139\n",
      "iter 8: train loss/sent=0.1514, time=7.77s\n",
      "iter 8: test acc=0.9084\n",
      "iter 9: train loss/sent=0.1105, time=7.61s\n",
      "iter 9: test acc=0.9107\n",
      "iter 10: train loss/sent=0.0718, time=7.61s\n",
      "iter 10: test acc=0.9151\n",
      "iter 11: train loss/sent=0.0741, time=8.16s\n",
      "iter 11: test acc=0.9099\n",
      "iter 12: train loss/sent=0.0668, time=9.51s\n",
      "iter 12: test acc=0.9122\n",
      "iter 13: train loss/sent=0.0627, time=8.36s\n",
      "iter 13: test acc=0.9144\n",
      "iter 14: train loss/sent=0.0720, time=8.75s\n",
      "iter 14: test acc=0.9137\n",
      "iter 15: train loss/sent=0.0661, time=8.37s\n",
      "iter 15: test acc=0.9188\n",
      "iter 16: train loss/sent=0.0616, time=8.92s\n",
      "iter 16: test acc=0.9113\n",
      "iter 17: train loss/sent=0.0564, time=7.96s\n",
      "iter 17: test acc=0.9153\n",
      "iter 18: train loss/sent=0.0514, time=8.06s\n",
      "iter 18: test acc=0.9069\n",
      "iter 19: train loss/sent=0.0389, time=8.61s\n",
      "iter 19: test acc=0.9098\n",
      "iter 20: train loss/sent=0.0497, time=8.78s\n",
      "iter 20: test acc=0.9160\n",
      "iter 21: train loss/sent=0.0433, time=8.17s\n",
      "iter 21: test acc=0.9144\n",
      "iter 22: train loss/sent=0.0392, time=8.08s\n",
      "iter 22: test acc=0.9144\n",
      "iter 23: train loss/sent=0.0390, time=7.68s\n",
      "iter 23: test acc=0.9069\n",
      "iter 24: train loss/sent=0.0412, time=7.68s\n",
      "iter 24: test acc=0.9102\n",
      "iter 25: train loss/sent=0.0432, time=7.68s\n",
      "iter 25: test acc=0.9063\n",
      "iter 26: train loss/sent=0.0356, time=7.70s\n",
      "iter 26: test acc=0.8972\n",
      "iter 27: train loss/sent=0.0329, time=7.66s\n",
      "iter 27: test acc=0.8901\n",
      "iter 28: train loss/sent=0.0319, time=7.66s\n",
      "iter 28: test acc=0.8940\n",
      "iter 29: train loss/sent=0.0387, time=7.67s\n",
      "iter 29: test acc=0.8959\n",
      "iter 30: train loss/sent=0.0343, time=7.66s\n",
      "iter 30: test acc=0.8995\n",
      "iter 31: train loss/sent=0.0333, time=7.69s\n",
      "iter 31: test acc=0.8962\n",
      "iter 32: train loss/sent=0.0316, time=7.68s\n",
      "iter 32: test acc=0.9025\n",
      "iter 33: train loss/sent=0.0300, time=7.66s\n",
      "iter 33: test acc=0.9048\n",
      "iter 34: train loss/sent=0.0275, time=7.67s\n",
      "iter 34: test acc=0.9066\n",
      "iter 35: train loss/sent=0.0271, time=7.68s\n",
      "iter 35: test acc=0.8983\n",
      "iter 36: train loss/sent=0.0279, time=7.68s\n",
      "iter 36: test acc=0.9048\n",
      "iter 37: train loss/sent=0.0250, time=7.70s\n",
      "iter 37: test acc=0.9058\n",
      "iter 38: train loss/sent=0.0251, time=7.67s\n",
      "iter 38: test acc=0.9066\n",
      "iter 39: train loss/sent=0.0238, time=7.70s\n",
      "iter 39: test acc=0.8986\n",
      "iter 40: train loss/sent=0.0233, time=7.67s\n",
      "iter 40: test acc=0.8998\n",
      "iter 41: train loss/sent=0.0224, time=7.68s\n",
      "iter 41: test acc=0.9066\n",
      "iter 42: train loss/sent=0.0219, time=7.66s\n",
      "iter 42: test acc=0.8952\n",
      "iter 43: train loss/sent=0.0212, time=7.68s\n",
      "iter 43: test acc=0.8995\n",
      "iter 44: train loss/sent=0.0208, time=7.67s\n",
      "iter 44: test acc=0.9154\n",
      "iter 45: train loss/sent=0.0215, time=7.67s\n",
      "iter 45: test acc=0.9077\n",
      "iter 46: train loss/sent=0.0200, time=7.69s\n",
      "iter 46: test acc=0.9089\n",
      "iter 47: train loss/sent=0.0201, time=7.68s\n",
      "iter 47: test acc=0.9050\n",
      "iter 48: train loss/sent=0.0196, time=7.70s\n",
      "iter 48: test acc=0.9059\n",
      "iter 49: train loss/sent=0.0192, time=7.68s\n",
      "iter 49: test acc=0.9092\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/2labels/model-1.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 61908 tokens with 3621 phrases; found: 2904 phrases; correct: 1021.\n",
      "accuracy:  91.16%; precision:  35.16%; recall:  28.20%; FB1:  31.30\n",
      "                 : precision:  35.16%; recall:  28.20%; FB1:  31.30  2904\n"
     ]
    }
   ],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/2labels/model-1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the 200 dimension pre-trained Glove embeddings to represent the words which are then passed through the BiLSTM to predict the NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 1min 33s, total: 3min 10s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the pre-trained Glove embeddings\n",
    "embeddings = {}\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.200d.txt\") as f:\n",
    "    for line in f:\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vec = split[1:]\n",
    "        embeddings[word] = vec\n",
    "    embedding_dim = 200\n",
    "    out = np.random.uniform(-0.8, 0.8, (nwords, embedding_dim))\n",
    "    for word, embed in embeddings.items():\n",
    "        embed_np = np.array(embed)\n",
    "        if word in w2i.keys():\n",
    "            out[w2i[word]] = embed_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = len(out[0])\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "W_emb.init_from_array(out)\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=0.0332, time=13.32s\n",
      "iter 0: test acc=0.8939\n",
      "iter 1: train loss/sent=0.0278, time=12.99s\n",
      "iter 1: test acc=0.9003\n",
      "iter 2: train loss/sent=0.0377, time=12.98s\n",
      "iter 2: test acc=0.8917\n",
      "iter 3: train loss/sent=0.0283, time=13.00s\n",
      "iter 3: test acc=0.8963\n",
      "iter 4: train loss/sent=0.0419, time=12.96s\n",
      "iter 4: test acc=0.8949\n",
      "iter 5: train loss/sent=0.0348, time=12.98s\n",
      "iter 5: test acc=0.8970\n",
      "iter 6: train loss/sent=0.0321, time=12.96s\n",
      "iter 6: test acc=0.9037\n",
      "iter 7: train loss/sent=0.0303, time=13.07s\n",
      "iter 7: test acc=0.9046\n",
      "iter 8: train loss/sent=0.0246, time=13.02s\n",
      "iter 8: test acc=0.9041\n",
      "iter 9: train loss/sent=0.0271, time=13.05s\n",
      "iter 9: test acc=0.9078\n",
      "iter 10: train loss/sent=0.0295, time=12.99s\n",
      "iter 10: test acc=0.9077\n",
      "iter 11: train loss/sent=0.0276, time=12.97s\n",
      "iter 11: test acc=0.9073\n",
      "iter 12: train loss/sent=0.0276, time=12.98s\n",
      "iter 12: test acc=0.9088\n",
      "iter 13: train loss/sent=0.0256, time=12.98s\n",
      "iter 13: test acc=0.9107\n",
      "iter 14: train loss/sent=0.0368, time=12.96s\n",
      "iter 14: test acc=0.9086\n",
      "iter 15: train loss/sent=0.0329, time=12.99s\n",
      "iter 15: test acc=0.9083\n",
      "iter 16: train loss/sent=0.0272, time=12.96s\n",
      "iter 16: test acc=0.9087\n",
      "iter 17: train loss/sent=0.0238, time=12.99s\n",
      "iter 17: test acc=0.9082\n",
      "iter 18: train loss/sent=0.0254, time=12.97s\n",
      "iter 18: test acc=0.9074\n",
      "iter 19: train loss/sent=0.0333, time=12.97s\n",
      "iter 19: test acc=0.9067\n",
      "iter 20: train loss/sent=0.0386, time=12.96s\n",
      "iter 20: test acc=0.9083\n",
      "iter 21: train loss/sent=0.0235, time=12.96s\n",
      "iter 21: test acc=0.9033\n",
      "iter 22: train loss/sent=0.0253, time=12.98s\n",
      "iter 22: test acc=0.9097\n",
      "iter 23: train loss/sent=0.0252, time=12.96s\n",
      "iter 23: test acc=0.9089\n",
      "iter 24: train loss/sent=0.0264, time=13.04s\n",
      "iter 24: test acc=0.9080\n",
      "iter 25: train loss/sent=0.0247, time=13.02s\n",
      "iter 25: test acc=0.9058\n",
      "iter 26: train loss/sent=0.0295, time=12.99s\n",
      "iter 26: test acc=0.9027\n",
      "iter 27: train loss/sent=0.0243, time=13.12s\n",
      "iter 27: test acc=0.9014\n",
      "iter 28: train loss/sent=0.0277, time=13.37s\n",
      "iter 28: test acc=0.9042\n",
      "iter 29: train loss/sent=0.0242, time=13.29s\n",
      "iter 29: test acc=0.9041\n",
      "iter 30: train loss/sent=0.0236, time=12.98s\n",
      "iter 30: test acc=0.9062\n",
      "iter 31: train loss/sent=0.0275, time=12.99s\n",
      "iter 31: test acc=0.9013\n",
      "iter 32: train loss/sent=0.0277, time=13.00s\n",
      "iter 32: test acc=0.8997\n",
      "iter 33: train loss/sent=0.0236, time=12.98s\n",
      "iter 33: test acc=0.9028\n",
      "iter 34: train loss/sent=0.0228, time=12.98s\n",
      "iter 34: test acc=0.9041\n",
      "iter 35: train loss/sent=0.0278, time=13.06s\n",
      "iter 35: test acc=0.9015\n",
      "iter 36: train loss/sent=0.0252, time=12.98s\n",
      "iter 36: test acc=0.8994\n",
      "iter 37: train loss/sent=0.0224, time=13.05s\n",
      "iter 37: test acc=0.9034\n",
      "iter 38: train loss/sent=0.0226, time=12.98s\n",
      "iter 38: test acc=0.9054\n",
      "iter 39: train loss/sent=0.0255, time=12.99s\n",
      "iter 39: test acc=0.9003\n",
      "iter 40: train loss/sent=0.0257, time=12.98s\n",
      "iter 40: test acc=0.9062\n",
      "iter 41: train loss/sent=0.0231, time=13.00s\n",
      "iter 41: test acc=0.9023\n",
      "iter 42: train loss/sent=0.0233, time=12.99s\n",
      "iter 42: test acc=0.9050\n",
      "iter 43: train loss/sent=0.0239, time=13.17s\n",
      "iter 43: test acc=0.9070\n",
      "iter 44: train loss/sent=0.0244, time=12.97s\n",
      "iter 44: test acc=0.9052\n",
      "iter 45: train loss/sent=0.0244, time=12.96s\n",
      "iter 45: test acc=0.9041\n",
      "iter 46: train loss/sent=0.0221, time=12.96s\n",
      "iter 46: test acc=0.9047\n",
      "iter 47: train loss/sent=0.0242, time=13.00s\n",
      "iter 47: test acc=0.9044\n",
      "iter 48: train loss/sent=0.0317, time=12.95s\n",
      "iter 48: test acc=0.9078\n",
      "iter 49: train loss/sent=0.0265, time=12.97s\n",
      "iter 49: test acc=0.9054\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-2.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 61908 tokens with 3621 phrases; found: 1488 phrases; correct: 362.\n",
      "accuracy:  90.25%; precision:  24.33%; recall:  10.00%; FB1:  14.17\n",
      "          company: precision:  59.20%; recall:  11.58%; FB1:  19.37  125\n",
      "         facility: precision:   3.12%; recall:   1.13%; FB1:   1.66  96\n",
      "          geo-loc: precision:  60.85%; recall:  16.12%; FB1:  25.49  235\n",
      "            movie: precision:   0.00%; recall:   0.00%; FB1:   0.00  14\n",
      "      musicartist: precision:  10.71%; recall:   3.06%; FB1:   4.76  56\n",
      "            other: precision:  36.52%; recall:  10.42%; FB1:  16.21  178\n",
      "           person: precision:   8.64%; recall:  12.55%; FB1:  10.24  729\n",
      "          product: precision:  24.14%; recall:   2.46%; FB1:   4.47  29\n",
      "       sportsteam: precision:   4.76%; recall:   0.66%; FB1:   1.16  21\n",
      "           tvshow: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n"
     ]
    }
   ],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
