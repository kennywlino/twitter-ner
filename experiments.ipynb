{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Neural Network Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook represents the various experiments we conducted in order to see what parameters and modifications worked best. Some of the experiments below may not be reported in the paper due to the space constraint and/or due to its low performance.\n",
    "\n",
    "All of the models below are structured roughly the same where embeddings are passed through the bi-LSTM and the output of the bi-LSMT is passed to the softmax layer. Thus each model is referred to by its embedding type; a short description is written under each model for clarity. Most of the models below are also the 10-label variant of the challenge unless indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings (trained from training set alone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system below is the most basic system that utilizes just word embeddings initialized and trained solely on the training data (e.g. not pre-trained embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=7.4537, time=8.34s\n",
      "iter 0: test acc=0.9074\n",
      "iter 1: train loss/sent=5.3288, time=7.71s\n",
      "iter 1: test acc=0.9085\n",
      "iter 2: train loss/sent=4.2602, time=8.04s\n",
      "iter 2: test acc=0.9113\n",
      "iter 3: train loss/sent=3.4010, time=8.43s\n",
      "iter 3: test acc=0.9111\n",
      "iter 4: train loss/sent=2.6634, time=7.99s\n",
      "iter 4: test acc=0.9122\n",
      "iter 5: train loss/sent=2.0325, time=8.69s\n",
      "iter 5: test acc=0.9126\n",
      "iter 6: train loss/sent=1.4901, time=8.86s\n",
      "iter 6: test acc=0.9123\n",
      "iter 7: train loss/sent=1.0708, time=8.37s\n",
      "iter 7: test acc=0.9130\n",
      "iter 8: train loss/sent=0.7558, time=8.27s\n",
      "iter 8: test acc=0.9131\n",
      "iter 9: train loss/sent=0.4685, time=8.61s\n",
      "iter 9: test acc=0.9125\n",
      "iter 10: train loss/sent=0.3234, time=8.53s\n",
      "iter 10: test acc=0.9127\n",
      "iter 11: train loss/sent=0.2138, time=8.52s\n",
      "iter 11: test acc=0.9129\n",
      "iter 12: train loss/sent=0.1567, time=8.28s\n",
      "iter 12: test acc=0.9130\n",
      "iter 13: train loss/sent=0.1143, time=8.58s\n",
      "iter 13: test acc=0.9136\n",
      "iter 14: train loss/sent=0.1108, time=8.30s\n",
      "iter 14: test acc=0.9144\n",
      "iter 15: train loss/sent=0.1055, time=8.46s\n",
      "iter 15: test acc=0.9138\n",
      "iter 16: train loss/sent=0.1018, time=8.25s\n",
      "iter 16: test acc=0.9143\n",
      "iter 17: train loss/sent=0.0786, time=8.39s\n",
      "iter 17: test acc=0.9141\n",
      "iter 18: train loss/sent=0.0800, time=8.59s\n",
      "iter 18: test acc=0.9148\n",
      "iter 19: train loss/sent=0.0719, time=8.21s\n",
      "iter 19: test acc=0.9149\n",
      "iter 20: train loss/sent=0.0739, time=8.05s\n",
      "iter 20: test acc=0.9143\n",
      "iter 21: train loss/sent=0.0741, time=8.30s\n",
      "iter 21: test acc=0.9150\n",
      "iter 22: train loss/sent=0.0683, time=8.22s\n",
      "iter 22: test acc=0.9152\n",
      "iter 23: train loss/sent=0.0670, time=8.24s\n",
      "iter 23: test acc=0.9144\n",
      "iter 24: train loss/sent=0.0707, time=8.60s\n",
      "iter 24: test acc=0.9152\n",
      "iter 25: train loss/sent=0.0777, time=8.51s\n",
      "iter 25: test acc=0.9150\n",
      "iter 26: train loss/sent=0.0684, time=8.60s\n",
      "iter 26: test acc=0.9156\n",
      "iter 27: train loss/sent=0.0604, time=8.20s\n",
      "iter 27: test acc=0.9153\n",
      "iter 28: train loss/sent=0.0611, time=8.29s\n",
      "iter 28: test acc=0.9148\n",
      "iter 29: train loss/sent=0.0627, time=8.46s\n",
      "iter 29: test acc=0.9153\n",
      "iter 30: train loss/sent=0.0618, time=8.47s\n",
      "iter 30: test acc=0.9158\n",
      "iter 31: train loss/sent=0.0572, time=8.43s\n",
      "iter 31: test acc=0.9155\n",
      "iter 32: train loss/sent=0.0579, time=8.28s\n",
      "iter 32: test acc=0.9152\n",
      "iter 33: train loss/sent=0.0533, time=8.18s\n",
      "iter 33: test acc=0.9151\n",
      "iter 34: train loss/sent=0.0533, time=8.10s\n",
      "iter 34: test acc=0.9153\n",
      "iter 35: train loss/sent=0.0469, time=8.28s\n",
      "iter 35: test acc=0.9152\n",
      "iter 36: train loss/sent=0.0448, time=8.06s\n",
      "iter 36: test acc=0.9161\n",
      "iter 37: train loss/sent=0.0465, time=7.93s\n",
      "iter 37: test acc=0.9159\n",
      "iter 38: train loss/sent=0.0452, time=7.92s\n",
      "iter 38: test acc=0.9158\n",
      "iter 39: train loss/sent=0.0432, time=7.93s\n",
      "iter 39: test acc=0.9155\n",
      "iter 40: train loss/sent=0.0426, time=7.92s\n",
      "iter 40: test acc=0.9154\n",
      "iter 41: train loss/sent=0.0424, time=7.93s\n",
      "iter 41: test acc=0.9157\n",
      "iter 42: train loss/sent=0.0383, time=7.94s\n",
      "iter 42: test acc=0.9156\n",
      "iter 43: train loss/sent=0.0440, time=8.06s\n",
      "iter 43: test acc=0.9152\n",
      "iter 44: train loss/sent=0.0391, time=8.15s\n",
      "iter 44: test acc=0.9155\n",
      "iter 45: train loss/sent=0.0374, time=8.11s\n",
      "iter 45: test acc=0.9162\n",
      "iter 46: train loss/sent=0.0366, time=8.10s\n",
      "iter 46: test acc=0.9160\n",
      "iter 47: train loss/sent=0.0342, time=8.11s\n",
      "iter 47: test acc=0.9163\n",
      "iter 48: train loss/sent=0.0356, time=8.09s\n",
      "iter 48: test acc=0.9155\n",
      "iter 49: train loss/sent=0.0357, time=8.09s\n",
      "iter 49: test acc=0.9155\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-1.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-1.txt > predicted/10labels/model-1-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 labels model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the two labels (NER vs. No NER) version of the above system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train_notypes\")\n",
    "dev = read_dataset(\"wnut17/data/dev_notypes\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test_notypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=4.5196, time=9.37s\n",
      "iter 0: test acc=0.9084\n",
      "iter 1: train loss/sent=2.9940, time=8.65s\n",
      "iter 1: test acc=0.9136\n",
      "iter 2: train loss/sent=2.2171, time=8.09s\n",
      "iter 2: test acc=0.9187\n",
      "iter 3: train loss/sent=1.5927, time=7.80s\n",
      "iter 3: test acc=0.9162\n",
      "iter 4: train loss/sent=1.0688, time=7.77s\n",
      "iter 4: test acc=0.9167\n",
      "iter 5: train loss/sent=0.6863, time=7.91s\n",
      "iter 5: test acc=0.9117\n",
      "iter 6: train loss/sent=0.4192, time=8.08s\n",
      "iter 6: test acc=0.9127\n",
      "iter 7: train loss/sent=0.2398, time=7.74s\n",
      "iter 7: test acc=0.9139\n",
      "iter 8: train loss/sent=0.1514, time=7.77s\n",
      "iter 8: test acc=0.9084\n",
      "iter 9: train loss/sent=0.1105, time=7.61s\n",
      "iter 9: test acc=0.9107\n",
      "iter 10: train loss/sent=0.0718, time=7.61s\n",
      "iter 10: test acc=0.9151\n",
      "iter 11: train loss/sent=0.0741, time=8.16s\n",
      "iter 11: test acc=0.9099\n",
      "iter 12: train loss/sent=0.0668, time=9.51s\n",
      "iter 12: test acc=0.9122\n",
      "iter 13: train loss/sent=0.0627, time=8.36s\n",
      "iter 13: test acc=0.9144\n",
      "iter 14: train loss/sent=0.0720, time=8.75s\n",
      "iter 14: test acc=0.9137\n",
      "iter 15: train loss/sent=0.0661, time=8.37s\n",
      "iter 15: test acc=0.9188\n",
      "iter 16: train loss/sent=0.0616, time=8.92s\n",
      "iter 16: test acc=0.9113\n",
      "iter 17: train loss/sent=0.0564, time=7.96s\n",
      "iter 17: test acc=0.9153\n",
      "iter 18: train loss/sent=0.0514, time=8.06s\n",
      "iter 18: test acc=0.9069\n",
      "iter 19: train loss/sent=0.0389, time=8.61s\n",
      "iter 19: test acc=0.9098\n",
      "iter 20: train loss/sent=0.0497, time=8.78s\n",
      "iter 20: test acc=0.9160\n",
      "iter 21: train loss/sent=0.0433, time=8.17s\n",
      "iter 21: test acc=0.9144\n",
      "iter 22: train loss/sent=0.0392, time=8.08s\n",
      "iter 22: test acc=0.9144\n",
      "iter 23: train loss/sent=0.0390, time=7.68s\n",
      "iter 23: test acc=0.9069\n",
      "iter 24: train loss/sent=0.0412, time=7.68s\n",
      "iter 24: test acc=0.9102\n",
      "iter 25: train loss/sent=0.0432, time=7.68s\n",
      "iter 25: test acc=0.9063\n",
      "iter 26: train loss/sent=0.0356, time=7.70s\n",
      "iter 26: test acc=0.8972\n",
      "iter 27: train loss/sent=0.0329, time=7.66s\n",
      "iter 27: test acc=0.8901\n",
      "iter 28: train loss/sent=0.0319, time=7.66s\n",
      "iter 28: test acc=0.8940\n",
      "iter 29: train loss/sent=0.0387, time=7.67s\n",
      "iter 29: test acc=0.8959\n",
      "iter 30: train loss/sent=0.0343, time=7.66s\n",
      "iter 30: test acc=0.8995\n",
      "iter 31: train loss/sent=0.0333, time=7.69s\n",
      "iter 31: test acc=0.8962\n",
      "iter 32: train loss/sent=0.0316, time=7.68s\n",
      "iter 32: test acc=0.9025\n",
      "iter 33: train loss/sent=0.0300, time=7.66s\n",
      "iter 33: test acc=0.9048\n",
      "iter 34: train loss/sent=0.0275, time=7.67s\n",
      "iter 34: test acc=0.9066\n",
      "iter 35: train loss/sent=0.0271, time=7.68s\n",
      "iter 35: test acc=0.8983\n",
      "iter 36: train loss/sent=0.0279, time=7.68s\n",
      "iter 36: test acc=0.9048\n",
      "iter 37: train loss/sent=0.0250, time=7.70s\n",
      "iter 37: test acc=0.9058\n",
      "iter 38: train loss/sent=0.0251, time=7.67s\n",
      "iter 38: test acc=0.9066\n",
      "iter 39: train loss/sent=0.0238, time=7.70s\n",
      "iter 39: test acc=0.8986\n",
      "iter 40: train loss/sent=0.0233, time=7.67s\n",
      "iter 40: test acc=0.8998\n",
      "iter 41: train loss/sent=0.0224, time=7.68s\n",
      "iter 41: test acc=0.9066\n",
      "iter 42: train loss/sent=0.0219, time=7.66s\n",
      "iter 42: test acc=0.8952\n",
      "iter 43: train loss/sent=0.0212, time=7.68s\n",
      "iter 43: test acc=0.8995\n",
      "iter 44: train loss/sent=0.0208, time=7.67s\n",
      "iter 44: test acc=0.9154\n",
      "iter 45: train loss/sent=0.0215, time=7.67s\n",
      "iter 45: test acc=0.9077\n",
      "iter 46: train loss/sent=0.0200, time=7.69s\n",
      "iter 46: test acc=0.9089\n",
      "iter 47: train loss/sent=0.0201, time=7.68s\n",
      "iter 47: test acc=0.9050\n",
      "iter 48: train loss/sent=0.0196, time=7.70s\n",
      "iter 48: test acc=0.9059\n",
      "iter 49: train loss/sent=0.0192, time=7.68s\n",
      "iter 49: test acc=0.9092\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/2labels/model-1.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/2labels/model-1.txt > predicted/2labels/model-1-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the 50 dimension pre-trained Glove embeddings to represent the words which are then passed through the BiLSTM to predict the NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 1min 33s, total: 3min 10s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the pre-trained Glove embeddings\n",
    "embeddings = {}\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.200d.txt\") as f:\n",
    "    for line in f:\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vec = split[1:]\n",
    "        embeddings[word] = vec\n",
    "    embedding_dim = len(embeddings[list(embeddings.keys())[0]])\n",
    "    out = np.random.uniform(-0.8, 0.8, (nwords, embedding_dim))\n",
    "    for word, embed in embeddings.items():\n",
    "        embed_np = np.array(embed)\n",
    "        if word in w2i.keys():\n",
    "            out[w2i[word]] = embed_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = len(out[0])\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "W_emb.init_from_array(out)\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=0.0332, time=13.32s\n",
      "iter 0: test acc=0.8939\n",
      "iter 1: train loss/sent=0.0278, time=12.99s\n",
      "iter 1: test acc=0.9003\n",
      "iter 2: train loss/sent=0.0377, time=12.98s\n",
      "iter 2: test acc=0.8917\n",
      "iter 3: train loss/sent=0.0283, time=13.00s\n",
      "iter 3: test acc=0.8963\n",
      "iter 4: train loss/sent=0.0419, time=12.96s\n",
      "iter 4: test acc=0.8949\n",
      "iter 5: train loss/sent=0.0348, time=12.98s\n",
      "iter 5: test acc=0.8970\n",
      "iter 6: train loss/sent=0.0321, time=12.96s\n",
      "iter 6: test acc=0.9037\n",
      "iter 7: train loss/sent=0.0303, time=13.07s\n",
      "iter 7: test acc=0.9046\n",
      "iter 8: train loss/sent=0.0246, time=13.02s\n",
      "iter 8: test acc=0.9041\n",
      "iter 9: train loss/sent=0.0271, time=13.05s\n",
      "iter 9: test acc=0.9078\n",
      "iter 10: train loss/sent=0.0295, time=12.99s\n",
      "iter 10: test acc=0.9077\n",
      "iter 11: train loss/sent=0.0276, time=12.97s\n",
      "iter 11: test acc=0.9073\n",
      "iter 12: train loss/sent=0.0276, time=12.98s\n",
      "iter 12: test acc=0.9088\n",
      "iter 13: train loss/sent=0.0256, time=12.98s\n",
      "iter 13: test acc=0.9107\n",
      "iter 14: train loss/sent=0.0368, time=12.96s\n",
      "iter 14: test acc=0.9086\n",
      "iter 15: train loss/sent=0.0329, time=12.99s\n",
      "iter 15: test acc=0.9083\n",
      "iter 16: train loss/sent=0.0272, time=12.96s\n",
      "iter 16: test acc=0.9087\n",
      "iter 17: train loss/sent=0.0238, time=12.99s\n",
      "iter 17: test acc=0.9082\n",
      "iter 18: train loss/sent=0.0254, time=12.97s\n",
      "iter 18: test acc=0.9074\n",
      "iter 19: train loss/sent=0.0333, time=12.97s\n",
      "iter 19: test acc=0.9067\n",
      "iter 20: train loss/sent=0.0386, time=12.96s\n",
      "iter 20: test acc=0.9083\n",
      "iter 21: train loss/sent=0.0235, time=12.96s\n",
      "iter 21: test acc=0.9033\n",
      "iter 22: train loss/sent=0.0253, time=12.98s\n",
      "iter 22: test acc=0.9097\n",
      "iter 23: train loss/sent=0.0252, time=12.96s\n",
      "iter 23: test acc=0.9089\n",
      "iter 24: train loss/sent=0.0264, time=13.04s\n",
      "iter 24: test acc=0.9080\n",
      "iter 25: train loss/sent=0.0247, time=13.02s\n",
      "iter 25: test acc=0.9058\n",
      "iter 26: train loss/sent=0.0295, time=12.99s\n",
      "iter 26: test acc=0.9027\n",
      "iter 27: train loss/sent=0.0243, time=13.12s\n",
      "iter 27: test acc=0.9014\n",
      "iter 28: train loss/sent=0.0277, time=13.37s\n",
      "iter 28: test acc=0.9042\n",
      "iter 29: train loss/sent=0.0242, time=13.29s\n",
      "iter 29: test acc=0.9041\n",
      "iter 30: train loss/sent=0.0236, time=12.98s\n",
      "iter 30: test acc=0.9062\n",
      "iter 31: train loss/sent=0.0275, time=12.99s\n",
      "iter 31: test acc=0.9013\n",
      "iter 32: train loss/sent=0.0277, time=13.00s\n",
      "iter 32: test acc=0.8997\n",
      "iter 33: train loss/sent=0.0236, time=12.98s\n",
      "iter 33: test acc=0.9028\n",
      "iter 34: train loss/sent=0.0228, time=12.98s\n",
      "iter 34: test acc=0.9041\n",
      "iter 35: train loss/sent=0.0278, time=13.06s\n",
      "iter 35: test acc=0.9015\n",
      "iter 36: train loss/sent=0.0252, time=12.98s\n",
      "iter 36: test acc=0.8994\n",
      "iter 37: train loss/sent=0.0224, time=13.05s\n",
      "iter 37: test acc=0.9034\n",
      "iter 38: train loss/sent=0.0226, time=12.98s\n",
      "iter 38: test acc=0.9054\n",
      "iter 39: train loss/sent=0.0255, time=12.99s\n",
      "iter 39: test acc=0.9003\n",
      "iter 40: train loss/sent=0.0257, time=12.98s\n",
      "iter 40: test acc=0.9062\n",
      "iter 41: train loss/sent=0.0231, time=13.00s\n",
      "iter 41: test acc=0.9023\n",
      "iter 42: train loss/sent=0.0233, time=12.99s\n",
      "iter 42: test acc=0.9050\n",
      "iter 43: train loss/sent=0.0239, time=13.17s\n",
      "iter 43: test acc=0.9070\n",
      "iter 44: train loss/sent=0.0244, time=12.97s\n",
      "iter 44: test acc=0.9052\n",
      "iter 45: train loss/sent=0.0244, time=12.96s\n",
      "iter 45: test acc=0.9041\n",
      "iter 46: train loss/sent=0.0221, time=12.96s\n",
      "iter 46: test acc=0.9047\n",
      "iter 47: train loss/sent=0.0242, time=13.00s\n",
      "iter 47: test acc=0.9044\n",
      "iter 48: train loss/sent=0.0317, time=12.95s\n",
      "iter 48: test acc=0.9078\n",
      "iter 49: train loss/sent=0.0265, time=12.97s\n",
      "iter 49: test acc=0.9054\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-2.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-2.txt > predicted/10labels/model-2-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character + Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a combination of character embeddings and word embeddings trained solely from the dataset (e.g. no pre-trained embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))   # word: its position in the doc; all unknown words(UNK) are in the position 0\n",
    "t2i = defaultdict(lambda: len(t2i))   # tag: its position {'b-sportsteam': 11, 'i-musicartist': 19...}\n",
    "char2i = defaultdict(lambda: len(char2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = w2i[\"<unk>\"]\n",
    "UNK_char = char2i[\"<unk_char>\"]\n",
    "pad_char = char2i[\"<*>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read and preprocess data for training\n",
    "# return: processed data\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "                for char in word:\n",
    "                    char2i[char]\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "char2i = defaultdict(lambda: UNK_char, char2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "nchar = max(char2i.values()) + 1\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# parameter sizes are not correct; input_dim = word_embedding_dim + 128?\n",
    "CH_EMB_SIZE = 30\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "\n",
    "chW_emb = model.add_lookup_parameters((nchar, CH_EMB_SIZE))  # char embeddings\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "\n",
    "char_lstm_builders = [dy.LSTMBuilder(1, 30, EMB_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, 30, EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "lstm_builders = [dy.LSTMBuilder(1, 128 + HID_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, 128 + HID_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "W_sm = model.add_parameters((ntags, 128))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# parameter sizes are not correct; input_dim = word_embedding_dim + 128?\n",
    "CH_EMB_SIZE = 30\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "\n",
    "chW_emb = model.add_lookup_parameters((nchar, CH_EMB_SIZE))  # char embeddings\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "\n",
    "char_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, EMB_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, CH_EMB_SIZE, EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "lstm_builders = [dy.LSTMBuilder(1, (2 * EMB_SIZE) + EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, (2 * EMB_SIZE) + EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_EMB = 128\n",
    "# HID = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in char_lstm_builders]\n",
    "    word_embs = dy.lookup(W_emb, word)\n",
    "    char_ids = [pad_char] + [char2i[c] for c in i2w(word)] + [pad_char]\n",
    "    char_embs = [dy.lookup(chW_emb, cid) for cid in char_ids]\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    return dy.concatenate([word_embs, complete_char_rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    word_embs = [word_representation(w) for w,t in sent]\n",
    "    \n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    " \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    word_embs = [word_representation(w) for w,t in sent]\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=6.2222, time=90.29s\n",
      "iter 0: test acc=0.9064\n",
      "iter 1: train loss/sent=4.4422, time=89.93s\n",
      "iter 1: test acc=0.9113\n",
      "iter 2: train loss/sent=3.6548, time=88.29s\n",
      "iter 2: test acc=0.9121\n",
      "iter 3: train loss/sent=3.0750, time=86.21s\n",
      "iter 3: test acc=0.9119\n",
      "iter 4: train loss/sent=2.5578, time=93.24s\n",
      "iter 4: test acc=0.9127\n",
      "iter 5: train loss/sent=2.0375, time=88.39s\n",
      "iter 5: test acc=0.9119\n",
      "iter 6: train loss/sent=1.5580, time=88.01s\n",
      "iter 6: test acc=0.9048\n",
      "iter 7: train loss/sent=1.1370, time=89.03s\n",
      "iter 7: test acc=0.9111\n",
      "iter 8: train loss/sent=0.8220, time=89.47s\n",
      "iter 8: test acc=0.9027\n",
      "iter 9: train loss/sent=0.5662, time=87.63s\n",
      "iter 9: test acc=0.9128\n",
      "iter 10: train loss/sent=0.3877, time=83.59s\n",
      "iter 10: test acc=0.9103\n",
      "iter 11: train loss/sent=0.2605, time=87.40s\n",
      "iter 11: test acc=0.9122\n",
      "iter 12: train loss/sent=0.1785, time=85.34s\n",
      "iter 12: test acc=0.9102\n",
      "iter 13: train loss/sent=0.1526, time=85.91s\n",
      "iter 13: test acc=0.9061\n",
      "iter 14: train loss/sent=0.1087, time=85.05s\n",
      "iter 14: test acc=0.9074\n",
      "iter 15: train loss/sent=0.1267, time=90.18s\n",
      "iter 15: test acc=0.9060\n",
      "iter 16: train loss/sent=0.1074, time=85.34s\n",
      "iter 16: test acc=0.9121\n",
      "iter 17: train loss/sent=0.0921, time=82.60s\n",
      "iter 17: test acc=0.8992\n",
      "iter 18: train loss/sent=0.0826, time=82.71s\n",
      "iter 18: test acc=0.9031\n",
      "iter 19: train loss/sent=0.0866, time=82.14s\n",
      "iter 19: test acc=0.9079\n",
      "iter 20: train loss/sent=0.0845, time=82.39s\n",
      "iter 20: test acc=0.9056\n",
      "iter 21: train loss/sent=0.0711, time=84.12s\n",
      "iter 21: test acc=0.9086\n",
      "iter 22: train loss/sent=0.0785, time=83.98s\n",
      "iter 22: test acc=0.9132\n",
      "iter 23: train loss/sent=0.0783, time=84.03s\n",
      "iter 23: test acc=0.8874\n",
      "iter 24: train loss/sent=0.0698, time=82.18s\n",
      "iter 24: test acc=0.9078\n",
      "iter 25: train loss/sent=0.0686, time=82.34s\n",
      "iter 25: test acc=0.9059\n",
      "iter 26: train loss/sent=0.0665, time=82.18s\n",
      "iter 26: test acc=0.9098\n",
      "iter 27: train loss/sent=0.0671, time=82.20s\n",
      "iter 27: test acc=0.9059\n",
      "iter 28: train loss/sent=0.0659, time=82.24s\n",
      "iter 28: test acc=0.9071\n",
      "iter 29: train loss/sent=0.0618, time=82.24s\n",
      "iter 29: test acc=0.9118\n",
      "iter 30: train loss/sent=0.0651, time=82.25s\n",
      "iter 30: test acc=0.9094\n",
      "iter 31: train loss/sent=0.0583, time=82.20s\n",
      "iter 31: test acc=0.9087\n",
      "iter 32: train loss/sent=0.0535, time=83.88s\n",
      "iter 32: test acc=0.9078\n",
      "iter 33: train loss/sent=0.0538, time=82.29s\n",
      "iter 33: test acc=0.9121\n",
      "iter 34: train loss/sent=0.0516, time=84.51s\n",
      "iter 34: test acc=0.9039\n",
      "iter 35: train loss/sent=0.0560, time=83.91s\n",
      "iter 35: test acc=0.9121\n",
      "iter 36: train loss/sent=0.0519, time=85.71s\n",
      "iter 36: test acc=0.9002\n",
      "iter 37: train loss/sent=0.0448, time=83.93s\n",
      "iter 37: test acc=0.9071\n",
      "iter 38: train loss/sent=0.0471, time=91.19s\n",
      "iter 38: test acc=0.9111\n",
      "iter 39: train loss/sent=0.0465, time=87.33s\n",
      "iter 39: test acc=0.9121\n",
      "iter 40: train loss/sent=0.0420, time=89.00s\n",
      "iter 40: test acc=0.9075\n",
      "iter 41: train loss/sent=0.0471, time=119.08s\n",
      "iter 41: test acc=0.9077\n",
      "iter 42: train loss/sent=0.0432, time=88.43s\n",
      "iter 42: test acc=0.9090\n",
      "iter 43: train loss/sent=0.0389, time=91.07s\n",
      "iter 43: test acc=0.9080\n",
      "iter 44: train loss/sent=0.0353, time=91.86s\n",
      "iter 44: test acc=0.9062\n",
      "iter 45: train loss/sent=0.0390, time=87.88s\n",
      "iter 45: test acc=0.9069\n",
      "iter 46: train loss/sent=0.0345, time=91.51s\n",
      "iter 46: test acc=0.9049\n",
      "iter 47: train loss/sent=0.0354, time=88.86s\n",
      "iter 47: test acc=0.9097\n",
      "iter 48: train loss/sent=0.0338, time=90.08s\n",
      "iter 48: test acc=0.9131\n",
      "iter 49: train loss/sent=0.0333, time=90.27s\n",
      "iter 49: test acc=0.9064\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-3.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-3.txt > predicted/10labels/model-3-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character + GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model uses character embeddings trained from the dataset and the 50 dimensional pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))   # word: its position in the doc; all unknown words(UNK) are in the position 0\n",
    "t2i = defaultdict(lambda: len(t2i))   # tag: its position {'b-sportsteam': 11, 'i-musicartist': 19...}\n",
    "char2i = defaultdict(lambda: len(char2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = w2i[\"<unk>\"]\n",
    "UNK_char = char2i[\"<unk_char>\"]\n",
    "pad_char = char2i[\"<*>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read and preprocess data for training\n",
    "# return: processed data\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "                for char in word:\n",
    "                    char2i[char]\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "char2i = defaultdict(lambda: UNK_char, char2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "nchar = max(char2i.values()) + 1\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 2.57 s, total: 25.9 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the pre-trained Glove embeddings\n",
    "embeddings = {}\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.50d.txt\") as f:\n",
    "    for line in f:\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vec = split[1:]\n",
    "        embeddings[word] = vec\n",
    "    embedding_dim = len(embeddings[list(embeddings.keys())[0]])\n",
    "    out = np.random.uniform(-0.8, 0.8, (nwords, embedding_dim))\n",
    "    for word, embed in embeddings.items():\n",
    "        embed_np = np.array(embed)\n",
    "        if word in w2i.keys():\n",
    "            out[w2i[word]] = embed_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# parameter sizes are not correct; input_dim = word_embedding_dim + 128?\n",
    "CH_EMB_SIZE = 30\n",
    "EMB_SIZE = len(out[0])\n",
    "HID_SIZE = 64\n",
    "\n",
    "chW_emb = model.add_lookup_parameters((nchar, CH_EMB_SIZE))  # char embeddings\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "W_emb.init_from_array(out)\n",
    "\n",
    "char_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, EMB_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, CH_EMB_SIZE, EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "lstm_builders = [dy.LSTMBuilder(1, (2 * EMB_SIZE) + EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, (2 * EMB_SIZE) + EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in char_lstm_builders]\n",
    "    word_embs = dy.lookup(W_emb, word)\n",
    "    char_ids = [pad_char] + [char2i[c] for c in i2w(word)] + [pad_char]\n",
    "    char_embs = [dy.lookup(chW_emb, cid) for cid in char_ids]\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    return dy.concatenate([word_embs, complete_char_rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    word_embs = [word_representation(w) for w,t in sent]\n",
    "    \n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    " \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    word_embs = [word_representation(w) for w,t in sent]\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=5.7647, time=87.53s\n",
      "iter 0: test acc=0.9082\n",
      "iter 1: train loss/sent=4.3639, time=88.14s\n",
      "iter 1: test acc=0.9087\n",
      "iter 2: train loss/sent=3.7422, time=88.66s\n",
      "iter 2: test acc=0.9006\n",
      "iter 3: train loss/sent=3.2336, time=86.89s\n",
      "iter 3: test acc=0.9042\n",
      "iter 4: train loss/sent=2.6910, time=83.75s\n",
      "iter 4: test acc=0.9046\n",
      "iter 5: train loss/sent=2.1805, time=84.21s\n",
      "iter 5: test acc=0.8774\n",
      "iter 6: train loss/sent=1.6962, time=83.79s\n",
      "iter 6: test acc=0.8477\n",
      "iter 7: train loss/sent=1.2387, time=84.02s\n",
      "iter 7: test acc=0.8324\n",
      "iter 8: train loss/sent=0.8904, time=83.74s\n",
      "iter 8: test acc=0.8232\n",
      "iter 9: train loss/sent=0.6249, time=83.88s\n",
      "iter 9: test acc=0.8614\n",
      "iter 10: train loss/sent=0.4012, time=83.89s\n",
      "iter 10: test acc=0.7747\n",
      "iter 11: train loss/sent=0.3070, time=81.96s\n",
      "iter 11: test acc=0.8565\n",
      "iter 12: train loss/sent=0.1929, time=81.70s\n",
      "iter 12: test acc=0.8666\n",
      "iter 13: train loss/sent=0.1584, time=82.31s\n",
      "iter 13: test acc=0.8048\n",
      "iter 14: train loss/sent=0.1459, time=82.37s\n",
      "iter 14: test acc=0.7701\n",
      "iter 15: train loss/sent=0.1374, time=81.88s\n",
      "iter 15: test acc=0.8238\n",
      "iter 16: train loss/sent=0.1165, time=90.99s\n",
      "iter 16: test acc=0.8126\n",
      "iter 17: train loss/sent=0.1237, time=86.88s\n",
      "iter 17: test acc=0.8160\n",
      "iter 18: train loss/sent=0.0854, time=89.97s\n",
      "iter 18: test acc=0.8191\n",
      "iter 19: train loss/sent=0.0956, time=85.46s\n",
      "iter 19: test acc=0.8194\n",
      "iter 20: train loss/sent=0.1026, time=86.16s\n",
      "iter 20: test acc=0.7743\n",
      "iter 21: train loss/sent=0.0826, time=86.52s\n",
      "iter 21: test acc=0.8021\n",
      "iter 22: train loss/sent=0.0845, time=86.31s\n",
      "iter 22: test acc=0.8362\n",
      "iter 23: train loss/sent=0.0776, time=85.58s\n",
      "iter 23: test acc=0.7902\n",
      "iter 24: train loss/sent=0.0742, time=89.35s\n",
      "iter 24: test acc=0.8045\n",
      "iter 25: train loss/sent=0.0609, time=89.63s\n",
      "iter 25: test acc=0.8438\n",
      "iter 26: train loss/sent=0.0688, time=87.34s\n",
      "iter 26: test acc=0.7921\n",
      "iter 27: train loss/sent=0.0692, time=88.34s\n",
      "iter 27: test acc=0.7597\n",
      "iter 28: train loss/sent=0.0667, time=86.01s\n",
      "iter 28: test acc=0.8160\n",
      "iter 29: train loss/sent=0.0560, time=88.10s\n",
      "iter 29: test acc=0.8594\n",
      "iter 30: train loss/sent=0.0504, time=87.79s\n",
      "iter 30: test acc=0.8409\n",
      "iter 31: train loss/sent=0.0589, time=83.91s\n",
      "iter 31: test acc=0.8237\n",
      "iter 32: train loss/sent=0.0581, time=88.24s\n",
      "iter 32: test acc=0.7863\n",
      "iter 33: train loss/sent=0.0486, time=88.39s\n",
      "iter 33: test acc=0.8597\n",
      "iter 34: train loss/sent=0.0501, time=89.02s\n",
      "iter 34: test acc=0.8306\n",
      "iter 35: train loss/sent=0.0512, time=93.09s\n",
      "iter 35: test acc=0.8301\n",
      "iter 36: train loss/sent=0.0471, time=86.36s\n",
      "iter 36: test acc=0.8208\n",
      "iter 37: train loss/sent=0.0442, time=86.36s\n",
      "iter 37: test acc=0.8011\n",
      "iter 38: train loss/sent=0.0410, time=84.90s\n",
      "iter 38: test acc=0.7948\n",
      "iter 39: train loss/sent=0.0456, time=85.06s\n",
      "iter 39: test acc=0.7690\n",
      "iter 40: train loss/sent=0.0415, time=84.45s\n",
      "iter 40: test acc=0.8021\n",
      "iter 41: train loss/sent=0.0429, time=84.40s\n",
      "iter 41: test acc=0.7679\n",
      "iter 42: train loss/sent=0.0374, time=87.33s\n",
      "iter 42: test acc=0.7618\n",
      "iter 43: train loss/sent=0.0391, time=87.90s\n",
      "iter 43: test acc=0.8038\n",
      "iter 44: train loss/sent=0.0372, time=91.67s\n",
      "iter 44: test acc=0.8116\n",
      "iter 45: train loss/sent=0.0352, time=84.70s\n",
      "iter 45: test acc=0.8258\n",
      "iter 46: train loss/sent=0.0366, time=82.09s\n",
      "iter 46: test acc=0.8074\n",
      "iter 47: train loss/sent=0.0352, time=81.65s\n",
      "iter 47: test acc=0.8091\n",
      "iter 48: train loss/sent=0.0324, time=81.71s\n",
      "iter 48: test acc=0.8281\n",
      "iter 49: train loss/sent=0.0323, time=82.40s\n",
      "iter 49: test acc=0.8143\n",
      "CPU times: user 1h 58min 25s, sys: 36.1 s, total: 1h 59min 1s\n",
      "Wall time: 1h 59min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-4.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-4.txt > predicted/10labels/model-4-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthographic character embeddings + orthographic word embeddings + regular character embeddings + regular word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following system utilizes a combination of the orthographic character embeddings, orthographic word embeddings, regular character embeddings and regular word embeddings.  \n",
    "\n",
    "To give a concrete example, if we have 'Beyonce' as a word in a sentence, then the orthorgraphic representation would be 'Cccccc', and thus we would look up the embeddings of each individual character in the representation, as well as the embeddings of 'Cccccc' as a whole. Then like normal, we would look up the embeddings of each individual character in 'Beyonce' and 'Beyonce' itself (should it be in the look up dictionary and training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))   # word: its position in the doc; all unknown words(UNK) are in the position 0\n",
    "t2i = defaultdict(lambda: len(t2i))   # tag: its position {'b-sportsteam': 11, 'i-musicartist': 19...}\n",
    "\n",
    "char2i = defaultdict(lambda: len(char2i))\n",
    "\n",
    "ortho_char2i = defaultdict(lambda: len(ortho_char2i))\n",
    "ortho_word2i = defaultdict(lambda: len(ortho_word2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "UNK_char = char2i[\"<unk_char>\"]\n",
    "pad_char = char2i[\"<*>\"]\n",
    "\n",
    "pad_ortho_char = ortho_char2i[\"<*>\"]\n",
    "UNK_ortho_word = ortho_word2i[\"<unk_ortho_word>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]\n",
    "\n",
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]\n",
    "\n",
    "def i2ortho_char(index):\n",
    "    \"\"\"Takes a character index and returns a character.\"\"\"\n",
    "    return list(ortho_char2i.keys())[list(ortho_char2i.values()).index(index)]\n",
    "\n",
    "def i2ortho_word(index):\n",
    "    \"\"\"Takes a character index and returns a character.\"\"\"\n",
    "    return list(ortho_word2i.keys())[list(ortho_word2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ortho_crep(char):\n",
    "    if char.isupper():\n",
    "        return \"C\"\n",
    "    if char.islower():\n",
    "        return \"c\"\n",
    "    if char.isdigit():\n",
    "        return \"n\"\n",
    "    else:\n",
    "        return \"p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read and preprocess data for training\n",
    "# return: processed data [(1, 0, 0), (2, 1, 0), (3, 2, 0), (4, 2, 0),\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        ortho_sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                #sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "                ortho_word = str()\n",
    "                for char in word:\n",
    "                    char2i[char]\n",
    "                    ortho_char = get_ortho_crep(char)\n",
    "                    ortho_word += ortho_char\n",
    "                    ortho_char2i[ortho_char]\n",
    "                sent_list.append((w2i[word], ortho_word2i[ortho_word], t2i[tag])) # (word index, orthographic word index, tag index)\n",
    "\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_dataset(\"train\")\n",
    "dev = read_dataset(\"dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "char2i = defaultdict(lambda: UNK_char, char2i)\n",
    "ortho_word2i = defaultdict(lambda: UNK_ortho_word, ortho_word2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "nchar = max(char2i.values()) + 1\n",
    "\n",
    "n_ortho_char = len(ortho_char2i)\n",
    "n_ortho_words = max(ortho_word2i.values()) + 1\n",
    "\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14879 94 5 2227 21\n"
     ]
    }
   ],
   "source": [
    "print(nwords, nchar, n_ortho_char, n_ortho_words, ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ortho_word2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# parameter sizes are not correct; input_dim = word_embedding_dim + 128?\n",
    "W_EMB_SIZE = 64\n",
    "CH_EMB_SIZE = 30\n",
    "ORTHO_CH_EMB_SIZE = 30\n",
    "ORTHO_EMB_SIZE = 50\n",
    "HID_SIZE = 64\n",
    "#CHAR_LSTM_OUTPUT_SIZE = 200\n",
    "\n",
    "W_emb = model.add_lookup_parameters((nwords, W_EMB_SIZE))  # Word embeddings\n",
    "chW_emb = model.add_lookup_parameters((nchar, CH_EMB_SIZE))  # char embeddings\n",
    "\n",
    "orthoW_emb = model.add_lookup_parameters((n_ortho_words, ORTHO_EMB_SIZE)) # ortho word embed\n",
    "orthoChar_emb = model.add_lookup_parameters((n_ortho_char, CH_EMB_SIZE)) # ortho char embed\n",
    "\n",
    "# Word-based biLSTM + WO_EMBED_SIZE + 4*CHAR_LSTM_OUTPUT_SIZE\n",
    "lstm_builders = [dy.LSTMBuilder(1, W_EMB_SIZE + ORTHO_EMB_SIZE + 4*ORTHO_CH_EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, W_EMB_SIZE + ORTHO_EMB_SIZE + 4*ORTHO_CH_EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "# Char-based biLSTM\n",
    "char_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model), \n",
    "                     dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "    \n",
    "# Ortho features biLSTM\n",
    "ortho_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model), \n",
    "                     dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "    \n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthoword_representation(word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in ortho_lstm_builders]\n",
    "    for b in ortho_lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5)\n",
    "        \n",
    "    ortho_word_embs = dy.lookup(orthoW_emb, word)\n",
    "    char_ids = [pad_ortho_char] + [ortho_char2i[c] for c in i2ortho_word(word)] + [pad_ortho_char]\n",
    "    char_embs = [dy.lookup(orthoChar_emb, cid) for cid in char_ids] # orth ch embeddings\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    #complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    #return dy.concatenate([ortho_word_embs, complete_char_rep])\n",
    "    return ortho_word_embs, fwd_embs[-1], bwd_embs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word, ortho_word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in char_lstm_builders]\n",
    "    for b in char_lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5)    \n",
    "    \n",
    "    word_embs = dy.lookup(W_emb, word) # word embeddings\n",
    "    char_ids = [pad_char] + [char2i[c] for c in i2w(word)] + [pad_char] # word characters\n",
    "    char_embs = [dy.lookup(chW_emb, cid) for cid in char_ids] # word ch embeddings\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    #complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    #get the ortho-char plus ortho-word rep\n",
    "    orthow_embs, ortho_fwd_embs, ortho_bwd_embs = orthoword_representation(ortho_word)\n",
    "    #orthow_embs, ortho_fwd_embs, ortho_bwd_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "\n",
    "    return dy.concatenate([word_embs, fwd_embs[-1], bwd_embs[-1], orthow_embs, ortho_fwd_embs, ortho_bwd_embs])\n",
    "    #return word_embs, fwd_embs[-1], bwd_embs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    #char_plus_word_embs = [word_representation(w, ow) for w, ow, t in sent]\n",
    "    #word_embs, ch_fwd_embs, ch_bwd_embs = [word_representation(w) for w, ow, t in sent]\n",
    "    #orthochar_plus_orthoword_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "    #orthow_embs, ortho_fwd_embs, ortho_bwd_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "    word_plus_ortho_rep_emb = [word_representation(w, ow) for w, ow, t in sent]\n",
    "    \n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    for b in lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5) \n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(word_plus_ortho_rep_emb)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_plus_ortho_rep_emb))\n",
    " \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, oword, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    word_embs = [word_representation(w, ow) for w,ow,t in sent]\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(word_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_embs))\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, oword, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=6.3065, time=145.71s\n",
      "iter 0: test acc=0.9078\n",
      "iter 1: train loss/sent=4.8337, time=143.21s\n",
      "iter 1: test acc=0.9087\n",
      "iter 2: train loss/sent=4.2075, time=145.96s\n",
      "iter 2: test acc=0.9119\n",
      "iter 3: train loss/sent=3.7988, time=144.18s\n",
      "iter 3: test acc=0.9117\n",
      "iter 4: train loss/sent=3.3812, time=144.50s\n",
      "iter 4: test acc=0.9142\n",
      "iter 5: train loss/sent=2.9973, time=148.52s\n",
      "iter 5: test acc=0.9141\n",
      "iter 6: train loss/sent=2.7645, time=148.46s\n",
      "iter 6: test acc=0.9152\n",
      "iter 7: train loss/sent=2.4300, time=145.52s\n",
      "iter 7: test acc=0.9146\n",
      "iter 8: train loss/sent=2.1645, time=148.27s\n",
      "iter 8: test acc=0.9155\n",
      "iter 9: train loss/sent=1.9521, time=138.29s\n",
      "iter 9: test acc=0.9151\n",
      "iter 10: train loss/sent=1.7231, time=141.85s\n",
      "iter 10: test acc=0.9159\n",
      "iter 11: train loss/sent=1.5715, time=138.68s\n",
      "iter 11: test acc=0.9155\n",
      "iter 12: train loss/sent=1.4081, time=143.40s\n",
      "iter 12: test acc=0.9155\n",
      "iter 13: train loss/sent=1.2548, time=144.46s\n",
      "iter 13: test acc=0.9160\n",
      "iter 14: train loss/sent=1.1375, time=148.76s\n",
      "iter 14: test acc=0.9150\n",
      "iter 15: train loss/sent=0.9869, time=157.30s\n",
      "iter 15: test acc=0.9146\n",
      "iter 16: train loss/sent=0.8893, time=157.36s\n",
      "iter 16: test acc=0.9150\n",
      "iter 17: train loss/sent=0.7816, time=151.98s\n",
      "iter 17: test acc=0.9133\n",
      "iter 18: train loss/sent=0.7322, time=148.19s\n",
      "iter 18: test acc=0.9152\n",
      "iter 19: train loss/sent=0.6428, time=143.80s\n",
      "iter 19: test acc=0.9146\n",
      "iter 20: train loss/sent=0.5733, time=143.32s\n",
      "iter 20: test acc=0.9152\n",
      "iter 21: train loss/sent=0.5178, time=151.86s\n",
      "iter 21: test acc=0.9153\n",
      "iter 22: train loss/sent=0.4870, time=149.24s\n",
      "iter 22: test acc=0.9147\n",
      "iter 23: train loss/sent=0.4375, time=143.36s\n",
      "iter 23: test acc=0.9161\n",
      "iter 24: train loss/sent=0.4192, time=145.58s\n",
      "iter 24: test acc=0.9145\n",
      "iter 25: train loss/sent=0.3564, time=153.30s\n",
      "iter 25: test acc=0.9130\n",
      "iter 26: train loss/sent=0.3444, time=143.97s\n",
      "iter 26: test acc=0.9127\n",
      "iter 27: train loss/sent=0.2819, time=144.02s\n",
      "iter 27: test acc=0.9118\n",
      "iter 28: train loss/sent=0.3143, time=152.75s\n",
      "iter 28: test acc=0.9096\n",
      "iter 29: train loss/sent=0.2575, time=142.83s\n",
      "iter 29: test acc=0.9123\n",
      "iter 30: train loss/sent=0.2339, time=161.65s\n",
      "iter 30: test acc=0.9129\n",
      "iter 31: train loss/sent=0.2622, time=165.27s\n",
      "iter 31: test acc=0.9107\n",
      "iter 32: train loss/sent=0.2270, time=175.08s\n",
      "iter 32: test acc=0.9126\n",
      "iter 33: train loss/sent=0.2437, time=161.09s\n",
      "iter 33: test acc=0.9119\n",
      "iter 34: train loss/sent=0.2091, time=157.10s\n",
      "iter 34: test acc=0.9121\n",
      "iter 35: train loss/sent=0.2284, time=143.44s\n",
      "iter 35: test acc=0.9152\n",
      "iter 36: train loss/sent=0.1944, time=150.71s\n",
      "iter 36: test acc=0.9136\n",
      "iter 37: train loss/sent=0.1684, time=146.48s\n",
      "iter 37: test acc=0.9136\n",
      "iter 38: train loss/sent=0.1874, time=145.38s\n",
      "iter 38: test acc=0.9131\n",
      "iter 39: train loss/sent=0.1950, time=139.09s\n",
      "iter 39: test acc=0.9141\n",
      "iter 40: train loss/sent=0.1630, time=138.92s\n",
      "iter 40: test acc=0.9134\n",
      "iter 41: train loss/sent=0.1542, time=142.78s\n",
      "iter 41: test acc=0.9153\n",
      "iter 42: train loss/sent=0.1700, time=145.74s\n",
      "iter 42: test acc=0.9137\n",
      "iter 43: train loss/sent=0.1644, time=146.58s\n",
      "iter 43: test acc=0.9140\n",
      "iter 44: train loss/sent=0.1569, time=146.36s\n",
      "iter 44: test acc=0.9146\n",
      "iter 45: train loss/sent=0.1414, time=134.36s\n",
      "iter 45: test acc=0.9134\n",
      "iter 46: train loss/sent=0.1473, time=143.04s\n",
      "iter 46: test acc=0.9125\n",
      "iter 47: train loss/sent=0.1425, time=136.80s\n",
      "iter 47: test acc=0.9111\n",
      "iter 48: train loss/sent=0.1616, time=137.86s\n",
      "iter 48: test acc=0.9130\n",
      "iter 49: train loss/sent=0.1425, time=140.66s\n",
      "iter 49: test acc=0.9142\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-5.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, oword, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 61908 tokens with 3621 phrases; found: 2163 phrases; correct: 657.\n",
      "accuracy:  91.22%; precision:  30.37%; recall:  18.14%; FB1:  22.72\n",
      "          company: precision:  55.95%; recall:  14.71%; FB1:  23.30  168\n",
      "         facility: precision:  17.82%; recall:  13.53%; FB1:  15.38  202\n",
      "          geo-loc: precision:  48.89%; recall:  34.72%; FB1:  40.61  630\n",
      "            movie: precision:   0.00%; recall:   0.00%; FB1:   0.00  45\n",
      "      musicartist: precision:   8.33%; recall:   5.10%; FB1:   6.33  120\n",
      "            other: precision:  23.57%; recall:  17.15%; FB1:  19.85  454\n",
      "           person: precision:  29.83%; recall:  14.14%; FB1:  19.19  238\n",
      "          product: precision:  13.91%; recall:   5.63%; FB1:   8.02  115\n",
      "       sportsteam: precision:   9.43%; recall:   9.93%; FB1:   9.68  159\n",
      "           tvshow: precision:   0.00%; recall:   0.00%; FB1:   0.00  32\n"
     ]
    }
   ],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-5.txt > predicted/10labels/model-5-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthographic character embeddings + orthographic word embeddings + regular character embeddings + regular word embeddings + 50% dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following system utilizes a combination of the orthographic character embeddings, orthographic word embeddings, regular character embeddings and regular word embeddings (like the previous system) + 50% dropout.\n",
    "\n",
    "To give a concrete example, if we have 'Beyonce' as a word in a sentence, then the orthorgraphic representation would be 'Cccccc', and thus we would look up the embeddings of each individual character in the representation, as well as the embeddings of 'Cccccc' as a whole. Then like normal, we would look up the embeddings of each individual character in 'Beyonce' and 'Beyonce' itself (should it be in the look up dictionary and training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))   # word: its position in the doc; all unknown words(UNK) are in the position 0\n",
    "t2i = defaultdict(lambda: len(t2i))   # tag: its position {'b-sportsteam': 11, 'i-musicartist': 19...}\n",
    "\n",
    "char2i = defaultdict(lambda: len(char2i))\n",
    "\n",
    "ortho_char2i = defaultdict(lambda: len(ortho_char2i))\n",
    "ortho_word2i = defaultdict(lambda: len(ortho_word2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "UNK_char = char2i[\"<unk_char>\"]\n",
    "pad_char = char2i[\"<*>\"]\n",
    "\n",
    "pad_ortho_char = ortho_char2i[\"<*>\"]\n",
    "UNK_ortho_word = ortho_word2i[\"<unk_ortho_word>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]\n",
    "\n",
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]\n",
    "\n",
    "def i2ortho_char(index):\n",
    "    \"\"\"Takes a character index and returns a character.\"\"\"\n",
    "    return list(ortho_char2i.keys())[list(ortho_char2i.values()).index(index)]\n",
    "\n",
    "def i2ortho_word(index):\n",
    "    \"\"\"Takes a character index and returns a character.\"\"\"\n",
    "    return list(ortho_word2i.keys())[list(ortho_word2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ortho_crep(char):\n",
    "    if char.isupper():\n",
    "        return \"C\"\n",
    "    if char.islower():\n",
    "        return \"c\"\n",
    "    if char.isdigit():\n",
    "        return \"n\"\n",
    "    else:\n",
    "        return \"p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read and preprocess data for training\n",
    "# return: processed data [(1, 0, 0), (2, 1, 0), (3, 2, 0), (4, 2, 0),\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        ortho_sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                #sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "                ortho_word = str()\n",
    "                for char in word:\n",
    "                    char2i[char]\n",
    "                    ortho_char = get_ortho_crep(char)\n",
    "                    ortho_word += ortho_char\n",
    "                    ortho_char2i[ortho_char]\n",
    "                sent_list.append((w2i[word], ortho_word2i[ortho_word], t2i[tag])) # (word index, orthographic word index, tag index)\n",
    "\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_dataset(\"train\")\n",
    "dev = read_dataset(\"dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "char2i = defaultdict(lambda: UNK_char, char2i)\n",
    "ortho_word2i = defaultdict(lambda: UNK_ortho_word, ortho_word2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "nchar = max(char2i.values()) + 1\n",
    "\n",
    "n_ortho_char = len(ortho_char2i)\n",
    "n_ortho_words = max(ortho_word2i.values()) + 1\n",
    "\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14879 94 5 2227 21\n"
     ]
    }
   ],
   "source": [
    "print(nwords, nchar, n_ortho_char, n_ortho_words, ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ortho_word2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# parameter sizes are not correct; input_dim = word_embedding_dim + 128?\n",
    "W_EMB_SIZE = 64\n",
    "CH_EMB_SIZE = 30\n",
    "ORTHO_CH_EMB_SIZE = 30\n",
    "ORTHO_EMB_SIZE = 50\n",
    "HID_SIZE = 64\n",
    "#CHAR_LSTM_OUTPUT_SIZE = 200\n",
    "\n",
    "W_emb = model.add_lookup_parameters((nwords, W_EMB_SIZE))  # Word embeddings\n",
    "chW_emb = model.add_lookup_parameters((nchar, CH_EMB_SIZE))  # char embeddings\n",
    "\n",
    "orthoW_emb = model.add_lookup_parameters((n_ortho_words, ORTHO_EMB_SIZE)) # ortho word embed\n",
    "orthoChar_emb = model.add_lookup_parameters((n_ortho_char, CH_EMB_SIZE)) # ortho char embed\n",
    "\n",
    "# Word-based biLSTM + WO_EMBED_SIZE + 4*CHAR_LSTM_OUTPUT_SIZE\n",
    "lstm_builders = [dy.LSTMBuilder(1, W_EMB_SIZE + ORTHO_EMB_SIZE + 4*ORTHO_CH_EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, W_EMB_SIZE + ORTHO_EMB_SIZE + 4*ORTHO_CH_EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "\n",
    "# Char-based biLSTM\n",
    "char_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model), \n",
    "                     dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "    \n",
    "# Ortho features biLSTM\n",
    "ortho_lstm_builders = [dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model), \n",
    "                     dy.LSTMBuilder(1, CH_EMB_SIZE, ORTHO_CH_EMB_SIZE, model)] # fwd and bwd LSTM\n",
    "    \n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthoword_representation(word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in ortho_lstm_builders]\n",
    "    for b in ortho_lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5)\n",
    "        \n",
    "    ortho_word_embs = dy.lookup(orthoW_emb, word)\n",
    "    char_ids = [pad_ortho_char] + [ortho_char2i[c] for c in i2ortho_word(word)] + [pad_ortho_char]\n",
    "    char_embs = [dy.lookup(orthoChar_emb, cid) for cid in char_ids] # orth ch embeddings\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    #complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    #return dy.concatenate([ortho_word_embs, complete_char_rep])\n",
    "    return ortho_word_embs, fwd_embs[-1], bwd_embs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word, ortho_word):\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in char_lstm_builders]\n",
    "    for b in char_lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5)    \n",
    "    \n",
    "    word_embs = dy.lookup(W_emb, word) # word embeddings\n",
    "    char_ids = [pad_char] + [char2i[c] for c in i2w(word)] + [pad_char] # word characters\n",
    "    char_embs = [dy.lookup(chW_emb, cid) for cid in char_ids] # word ch embeddings\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(char_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(char_embs))\n",
    "    #complete_char_rep = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) \n",
    "    \n",
    "    #get the ortho-char plus ortho-word rep\n",
    "    orthow_embs, ortho_fwd_embs, ortho_bwd_embs = orthoword_representation(ortho_word)\n",
    "    #orthow_embs, ortho_fwd_embs, ortho_bwd_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "\n",
    "    return dy.concatenate([word_embs, fwd_embs[-1], bwd_embs[-1], orthow_embs, ortho_fwd_embs, ortho_bwd_embs])\n",
    "    #return word_embs, fwd_embs[-1], bwd_embs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    #char_plus_word_embs = [word_representation(w, ow) for w, ow, t in sent]\n",
    "    #word_embs, ch_fwd_embs, ch_bwd_embs = [word_representation(w) for w, ow, t in sent]\n",
    "    #orthochar_plus_orthoword_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "    #orthow_embs, ortho_fwd_embs, ortho_bwd_embs = [orthoword_representation(ow) for w, ow, t in sent]\n",
    "    word_plus_ortho_rep_emb = [word_representation(w, ow) for w, ow, t in sent]\n",
    "    \n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    for b in lstm_builders:\n",
    "        b.set_dropouts(0.5, 0.5) \n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(word_plus_ortho_rep_emb)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_plus_ortho_rep_emb))\n",
    " \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, oword, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    word_embs = [word_representation(w, ow) for w,ow,t in sent]\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    \n",
    "    fwd_embs = fwd_init.transduce(word_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_embs))\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, oword, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=6.3065, time=145.71s\n",
      "iter 0: test acc=0.9078\n",
      "iter 1: train loss/sent=4.8337, time=143.21s\n",
      "iter 1: test acc=0.9087\n",
      "iter 2: train loss/sent=4.2075, time=145.96s\n",
      "iter 2: test acc=0.9119\n",
      "iter 3: train loss/sent=3.7988, time=144.18s\n",
      "iter 3: test acc=0.9117\n",
      "iter 4: train loss/sent=3.3812, time=144.50s\n",
      "iter 4: test acc=0.9142\n",
      "iter 5: train loss/sent=2.9973, time=148.52s\n",
      "iter 5: test acc=0.9141\n",
      "iter 6: train loss/sent=2.7645, time=148.46s\n",
      "iter 6: test acc=0.9152\n",
      "iter 7: train loss/sent=2.4300, time=145.52s\n",
      "iter 7: test acc=0.9146\n",
      "iter 8: train loss/sent=2.1645, time=148.27s\n",
      "iter 8: test acc=0.9155\n",
      "iter 9: train loss/sent=1.9521, time=138.29s\n",
      "iter 9: test acc=0.9151\n",
      "iter 10: train loss/sent=1.7231, time=141.85s\n",
      "iter 10: test acc=0.9159\n",
      "iter 11: train loss/sent=1.5715, time=138.68s\n",
      "iter 11: test acc=0.9155\n",
      "iter 12: train loss/sent=1.4081, time=143.40s\n",
      "iter 12: test acc=0.9155\n",
      "iter 13: train loss/sent=1.2548, time=144.46s\n",
      "iter 13: test acc=0.9160\n",
      "iter 14: train loss/sent=1.1375, time=148.76s\n",
      "iter 14: test acc=0.9150\n",
      "iter 15: train loss/sent=0.9869, time=157.30s\n",
      "iter 15: test acc=0.9146\n",
      "iter 16: train loss/sent=0.8893, time=157.36s\n",
      "iter 16: test acc=0.9150\n",
      "iter 17: train loss/sent=0.7816, time=151.98s\n",
      "iter 17: test acc=0.9133\n",
      "iter 18: train loss/sent=0.7322, time=148.19s\n",
      "iter 18: test acc=0.9152\n",
      "iter 19: train loss/sent=0.6428, time=143.80s\n",
      "iter 19: test acc=0.9146\n",
      "iter 20: train loss/sent=0.5733, time=143.32s\n",
      "iter 20: test acc=0.9152\n",
      "iter 21: train loss/sent=0.5178, time=151.86s\n",
      "iter 21: test acc=0.9153\n",
      "iter 22: train loss/sent=0.4870, time=149.24s\n",
      "iter 22: test acc=0.9147\n",
      "iter 23: train loss/sent=0.4375, time=143.36s\n",
      "iter 23: test acc=0.9161\n",
      "iter 24: train loss/sent=0.4192, time=145.58s\n",
      "iter 24: test acc=0.9145\n",
      "iter 25: train loss/sent=0.3564, time=153.30s\n",
      "iter 25: test acc=0.9130\n",
      "iter 26: train loss/sent=0.3444, time=143.97s\n",
      "iter 26: test acc=0.9127\n",
      "iter 27: train loss/sent=0.2819, time=144.02s\n",
      "iter 27: test acc=0.9118\n",
      "iter 28: train loss/sent=0.3143, time=152.75s\n",
      "iter 28: test acc=0.9096\n",
      "iter 29: train loss/sent=0.2575, time=142.83s\n",
      "iter 29: test acc=0.9123\n",
      "iter 30: train loss/sent=0.2339, time=161.65s\n",
      "iter 30: test acc=0.9129\n",
      "iter 31: train loss/sent=0.2622, time=165.27s\n",
      "iter 31: test acc=0.9107\n",
      "iter 32: train loss/sent=0.2270, time=175.08s\n",
      "iter 32: test acc=0.9126\n",
      "iter 33: train loss/sent=0.2437, time=161.09s\n",
      "iter 33: test acc=0.9119\n",
      "iter 34: train loss/sent=0.2091, time=157.10s\n",
      "iter 34: test acc=0.9121\n",
      "iter 35: train loss/sent=0.2284, time=143.44s\n",
      "iter 35: test acc=0.9152\n",
      "iter 36: train loss/sent=0.1944, time=150.71s\n",
      "iter 36: test acc=0.9136\n",
      "iter 37: train loss/sent=0.1684, time=146.48s\n",
      "iter 37: test acc=0.9136\n",
      "iter 38: train loss/sent=0.1874, time=145.38s\n",
      "iter 38: test acc=0.9131\n",
      "iter 39: train loss/sent=0.1950, time=139.09s\n",
      "iter 39: test acc=0.9141\n",
      "iter 40: train loss/sent=0.1630, time=138.92s\n",
      "iter 40: test acc=0.9134\n",
      "iter 41: train loss/sent=0.1542, time=142.78s\n",
      "iter 41: test acc=0.9153\n",
      "iter 42: train loss/sent=0.1700, time=145.74s\n",
      "iter 42: test acc=0.9137\n",
      "iter 43: train loss/sent=0.1644, time=146.58s\n",
      "iter 43: test acc=0.9140\n",
      "iter 44: train loss/sent=0.1569, time=146.36s\n",
      "iter 44: test acc=0.9146\n",
      "iter 45: train loss/sent=0.1414, time=134.36s\n",
      "iter 45: test acc=0.9134\n",
      "iter 46: train loss/sent=0.1473, time=143.04s\n",
      "iter 46: test acc=0.9125\n",
      "iter 47: train loss/sent=0.1425, time=136.80s\n",
      "iter 47: test acc=0.9111\n",
      "iter 48: train loss/sent=0.1616, time=137.86s\n",
      "iter 48: test acc=0.9130\n",
      "iter 49: train loss/sent=0.1425, time=140.66s\n",
      "iter 49: test acc=0.9142\n"
     ]
    }
   ],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-6.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, oword, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 61908 tokens with 3621 phrases; found: 2163 phrases; correct: 657.\n",
      "accuracy:  91.22%; precision:  30.37%; recall:  18.14%; FB1:  22.72\n",
      "          company: precision:  55.95%; recall:  14.71%; FB1:  23.30  168\n",
      "         facility: precision:  17.82%; recall:  13.53%; FB1:  15.38  202\n",
      "          geo-loc: precision:  48.89%; recall:  34.72%; FB1:  40.61  630\n",
      "            movie: precision:   0.00%; recall:   0.00%; FB1:   0.00  45\n",
      "      musicartist: precision:   8.33%; recall:   5.10%; FB1:   6.33  120\n",
      "            other: precision:  23.57%; recall:  17.15%; FB1:  19.85  454\n",
      "           person: precision:  29.83%; recall:  14.14%; FB1:  19.19  238\n",
      "          product: precision:  13.91%; recall:   5.63%; FB1:   8.02  115\n",
      "       sportsteam: precision:   9.43%; recall:   9.93%; FB1:   9.68  159\n",
      "           tvshow: precision:   0.00%; recall:   0.00%; FB1:   0.00  32\n"
     ]
    }
   ],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-6.txt > predicted/10labels/model-6-eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the 200 dimension pre-trained word2vec embeddings from Godin et al. 2015 to represent the words which are then passed through the BiLSTM to predict the NER labels.\n",
    "\n",
    "This was not fully ran as the 200d embeddings ran too slowly per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dicts to convert words and tags into indices\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2w(index):\n",
    "    \"\"\"Takes a word index and returns word.\"\"\" \n",
    "    return list(w2i.keys())[list(w2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2t(index):\n",
    "    \"\"\"Takes a tag index and returns a tag.\"\"\"\n",
    "    return list(t2i.keys())[list(t2i.values()).index(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"Reads a file from the WNUT17 dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list containing each sentence from the dataset in a separate list.\n",
    "        Each element inside the sentence list is a tuple containing the\n",
    "        word index and tag index.\n",
    "        \n",
    "    For example:\n",
    "    [[(1, 0), (2, 1), (3, 2), (4, 0)],[(2,1), (13,2), (14, 0), (15,0)]]\n",
    "\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_list = []\n",
    "        sent_list = []\n",
    "        for line in f:\n",
    "            if len(line.strip()) != 0:\n",
    "                word, tag = line.strip().split(\"\\t\")\n",
    "                sent_list.append((w2i[word], t2i[tag])) # (word index, tag index)\n",
    "            else:\n",
    "                if len(sent_list) != 0:\n",
    "                    data_list.append(sent_list)\n",
    "                sent_list = []\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the training and dev data; combines them both as train\n",
    "train = read_dataset(\"wnut17/data/train\")\n",
    "dev = read_dataset(\"wnut17/data/dev\")\n",
    "train = train + dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezes the w2i dict\n",
    "w2i = defaultdict(lambda: UNK, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and number of tags\n",
    "nwords = max(w2i.values()) + 1 # used to exclude extra UNK\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_twitter_model import word2vecReader\n",
    "model = word2vecReader.Word2Vec().load_word2vec_format(\"word2vec_twitter_model/word2vec_twitter_model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3039345"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the test data;\n",
    "test = read_dataset(\"wnut17/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = len(out[0])\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "W_emb.init_from_array(out)\n",
    "lstm_builders = [dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model), \n",
    "                 dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)] # fwd and bwd LSTM\n",
    "W_sm = model.add_parameters((ntags, 2 * HID_SIZE))  # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @return list of error for each tag\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    errs = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of a single word from LSTM\n",
    "        predicted = W_sm_exp * complete_rep + b_sm_exp\n",
    "        err = dy.pickneglogsoftmax(predicted, tag)\n",
    "        errs.append(err)\n",
    "    return dy.esum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    \"\"\"\n",
    "    Builds the comp graph for the model with:\n",
    "    * Embeddings\n",
    "    * BiLSTM\n",
    "    @ return list of (word, predicted labels)\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    fwd_init, bwd_init = [b.initial_state() for b in lstm_builders]\n",
    "    word_embs = [dy.lookup(W_emb, word) for word, tag in sent]\n",
    "    \n",
    "    fwd_embs = [x.output() for x in fwd_init.add_inputs(word_embs)]\n",
    "    bwd_embs = [x.output() for x in bwd_init.add_inputs(reversed(word_embs))]\n",
    "    \n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for (word, tag), f_rep, b_rep in zip(sent, fwd_embs, reversed(bwd_embs)):\n",
    "        complete_rep = dy.concatenate([f_rep, b_rep]) # complete rep of word from LSTM\n",
    "        scores = (W_sm_exp * complete_rep + b_sm_exp).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        predicted_labels.append((word, predict))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_max = 50 # max num. of iterations\n",
    "\n",
    "for ITER in range(iter_max):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the defined computational graph.\n",
    "    Prints the accuracy per iteration and outputs the predictions in the final\n",
    "    iteration.\n",
    "    \n",
    "    Args:\n",
    "        iter_max: the maximum number of iterations to train the network.\n",
    "    \"\"\"\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    if ITER == iter_max - 1: # final iteration\n",
    "        out = open('predicted/10labels/model-X.txt', 'w')\n",
    "    for sent in train:\n",
    "        sent_error = build_tagging_graph(sent)\n",
    "        train_loss += sent_error.value()\n",
    "        sent_error.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    for sent in test:\n",
    "        p_labels = tag_sent(sent)\n",
    "        g_labels = [tags for word, tags in sent]\n",
    "        test_correct = 0\n",
    "        for i, p_g in enumerate(zip(p_labels, g_labels)):\n",
    "            word = p_g[0][0]\n",
    "            predicted = p_g[0][1]\n",
    "            gold = p_g[1]\n",
    "            if predicted == gold:\n",
    "                test_correct += 1\n",
    "            if ITER == iter_max - 1:\n",
    "                out.write(i2w(word) + '\\t' + i2t(gold) + '\\t' + i2t(predicted) + '\\n') # writes the word, gold tag, and predicted tag\n",
    "                if i == (len(p_g) - 1):\n",
    "                    out.write('\\n')\n",
    "        total_acc += test_correct / len(g_labels)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, total_acc / len(test)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs conll evaluation script; minor change for python3 in file (line 155)\n",
    "!python3 conlleval.py predicted/10labels/model-X.txt > predicted/10labels/model-X-eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
