
@article{plank2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.07836},
  primaryClass = {cs},
  title = {What to Do about Non-Standard (or Non-Canonical) Language in {{NLP}}},
  abstract = {Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community's approach to language. I argue for leveraging what I call fortuitous data, i.e., non-obvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.},
  journal = {arXiv:1608.07836 [cs]},
  author = {Plank, Barbara},
  month = aug,
  year = {2016},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/kennylino/Documents/Articles/PlankB/2016/Plank_2016_What to do about non-standard (or non-canonical) language in NLP.pdf;/Users/kennylino/Zotero/storage/RWGMFVKE/1608.html}
}

@article{holland1999,
  title = {Preliminary {{Tests}} of {{Language Learning}} in a {{Speech}}-{{Interactive Graphics Microworld}}},
  volume = {16},
  abstract = {A speech-interactive graphics microworld is described in which learners speak problem-solving directions to an animated agent and in which new scenarios can be authored. A proof-of-concept application is illustrated for sustaining basic speaking skill in Modern Standard Arabic. Preliminary tests of the application are summarized involving learners from both university and military settings. Problems are discussed in predicting and measuring learning gains based on brief exposure to new technologies.},
  number = {3},
  journal = {CALICO Journal},
  author = {Holland, V Melissa and Kaplan, Jonathan D and Sabol, Mark A},
  year = {1999},
  pages = {22},
  file = {/Users/kennylino/Documents/Articles/HollandV/undefined/Holland_Preliminary Tests of Language Learning in a Speech-Interactive Graphics.pdf}
}

@article{munro1999,
  title = {Foreign {{Accent}}, {{Comprehensibility}}, and {{Intelligibility}} in the {{Speech}} of {{Second Language Learners}}},
  volume = {49},
  issn = {00238333},
  doi = {10.1111/0023-8333.49.s1.8},
  language = {en},
  journal = {Language Learning},
  author = {Munro, Murray J. and Derwing, Tracey M.},
  year = {1999},
  pages = {285-310},
  file = {/Users/kennylino/Documents/Articles/MunroM/1999/Munro_1999_Foreign Accent, Comprehensibility, and Intelligibility in the Speech of Second.pdf}
}

@article{hardison2013,
  title = {Contextualized {{Computer}}-Based {{L2 Prosody Training}}: {{Evaluating}} the {{Effects}} of {{Discourse Context}} and {{Video Input}}},
  volume = {22},
  issn = {2056-9017},
  shorttitle = {Contextualized {{Computer}}-Based {{L2 Prosody Training}}},
  doi = {10.1558/cj.v22i2.175-190},
  abstract = {Two types of contextualized input in prosody training were investigated for 28 advanced L2 speakers of English (L1 Chinese). Their oral presentations provided training materials. Native-speakers (NSs) of English provided global prosody ratings, and participants completed questionnaires on perceived training effectiveness. Two groups received training input using Anvil, a web-based annotation tool integrating the video of a speech event with visual displays of the pitch contour, and practiced with Real-Time Pitch (RTP) in Computerized Speech Lab including feedback from a NS. Two groups used only RTP to view their pitch contours and practiced with the same feedback. Within these pairs, one group received discourse-level input and the other individual sentences. Each group served as its own control in a time-series design. All had comparable levels of performance prior to training. Results indicated that although all groups improved as a result of training, discourse-level input produced better transfer to novel natural discourse. The presence of video was more helpful with discourselevel input than with individual sentences. Speech samples collected 1 week after training revealed sustained improvement. Questionnaire results support the use of computer-based tools and authentic speech samples. Findings strongly suggest that meaningful contextualized input is valuable in prosody training when the measurement is at the level of extended connected speech typical of natural discourse.},
  number = {2},
  journal = {CALICO Journal},
  author = {Hardison, Debra M.},
  month = jan,
  year = {2013},
  pages = {175-190},
  file = {/Users/kennylino/Documents/Articles/HardisonD/2013/Hardison_2013_Contextualized Computer-based L2 Prosody Training.pdf}
}

@article{thompson2004,
  title = {Decoding Speech Prosody: {{Do}} Music Lessons Help?},
  volume = {4},
  issn = {1931-1516, 1528-3542},
  shorttitle = {Decoding Speech Prosody},
  doi = {10.1037/1528-3542.4.1.46},
  language = {en},
  number = {1},
  journal = {Emotion},
  author = {Thompson, William Forde and Schellenberg, E. Glenn and Husain, Gabriela},
  year = {2004},
  pages = {46-64},
  file = {/Users/kennylino/Documents/Articles/ThompsonW/2004/Thompson_2004_Decoding speech prosody.pdf}
}

@inproceedings{eskenazi1996,
  title = {Detection of Foreign Speakers' Pronunciation Errors for Second Language Training-Preliminary Results},
  volume = {3},
  isbn = {978-0-7803-3555-4},
  doi = {10.1109/ICSLP.1996.607892},
  publisher = {{IEEE}},
  author = {Eskenazi, M.},
  year = {1996},
  pages = {1465-1468},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/1996/Eskenazi_1996_Detection of foreign speakers' pronunciation errors for second language.pdf}
}

@article{eskenazi2009,
  title = {An Overview of Spoken Language Technology for Education},
  volume = {51},
  issn = {01676393},
  doi = {10.1016/j.specom.2009.04.005},
  abstract = {This paper reviews research in spoken language technology for education and more specifically for language learning. It traces the history of the domain and then groups main issues in the interaction with the student. It addresses the modalities of interaction and their implementation issues and algorithms. Then it discusses one user population \textendash{} children \textendash{} and an application for them. Finally it has a discussion of overall systems. It can be used as an introduction to the field and a source of reference materials.},
  language = {en},
  number = {10},
  journal = {Speech Communication},
  author = {Eskenazi, Maxine},
  month = oct,
  year = {2009},
  keywords = {read},
  pages = {832-844},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/2009/Eskenazi_2009_An overview of spoken language technology for education.pdf}
}

@article{hwang2016,
  title = {Evaluating Listening and Speaking Skills in a Mobile Game-Based Learning Environment with Situational Contexts},
  volume = {29},
  issn = {0958-8221, 1744-3210},
  doi = {10.1080/09588221.2015.1016438},
  language = {en},
  number = {4},
  journal = {Computer Assisted Language Learning},
  author = {Hwang, Wu-Yuin and Shih, Timothy K. and Ma, Zhao-Heng and Shadiev, Rustam and Chen, Shu-Yu},
  month = may,
  year = {2016},
  pages = {639-657},
  file = {/Users/kennylino/Documents/Articles/HwangW/2016/Hwang_2016_Evaluating listening and speaking skills in a mobile game-based learning.pdf}
}

@article{felps2009,
  title = {Foreign Accent Conversion in Computer Assisted Pronunciation Training},
  volume = {51},
  issn = {01676393},
  doi = {10.1016/j.specom.2008.11.004},
  abstract = {Learners of a second language practice their pronunciation by listening to and imitating utterances from native speakers. Recent research has shown that choosing a well-matched native speaker to imitate can have a positive impact on pronunciation training. Here we propose a voicetransformation technique that can be used to generate the (arguably) ideal voice to imitate: the own voice of the learner with a native accent. Our work extends previous research, which suggests that providing learners with prosodically corrected versions of their utterances can be a suitable form of feedback in computer assisted pronunciation training. Our technique provides a conversion of both prosodic and segmental characteristics by means of a pitch-synchronous decomposition of speech into glottal excitation and spectral envelope. We apply the technique to a corpus containing parallel recordings of foreign-accented and native-accented utterances, and validate the resulting accent conversions through a series of perceptual experiments. Our results indicate that the technique can reduce foreign accentedness without significantly altering the voice quality properties of the foreign speaker. Finally, we propose a pedagogical strategy for integrating accent conversion as a form of behavioral shaping in computer assisted pronunciation training.},
  language = {en},
  number = {10},
  journal = {Speech Communication},
  author = {Felps, Daniel and Bortfeld, Heather and Gutierrez-Osuna, Ricardo},
  month = oct,
  year = {2009},
  pages = {920-932},
  file = {/Users/kennylino/Documents/Articles/FelpsD/2009/Felps_2009_Foreign accent conversion in computer assisted pronunciation training.pdf}
}

@incollection{cardoso2017,
  title = {Can an Interactive Digital Game Help {{French}} Learners Improve Their Pronunciation?},
  isbn = {978-2-490-05704-7},
  abstract = {This study examines the effects of the pedagogical use of an interactive mobile digital game, Pr{\^e}t {\`a} N{\'e}gocier (P{\`a}N), on improving learners' pronunciation of French as a Second Language (FSL), using three holistic measures: comprehensibility, fluency, and overall pronunciation. Two groups of FSL learners engaged in different types of game-playing over one month: while the experimental group played P{\`a}N, the control group engaged in paper-based gamified information gap activities. Following a pre-test/post-test research design, our findings revealed no statistically significant differences between the two groups.},
  booktitle = {{{CALL}} in a Climate of Change: Adapting to Turbulent Global Conditions \textendash{} Short Papers from {{EUROCALL}} 2017},
  publisher = {{Research-publishing.net}},
  author = {Cardoso, Walcir and Rueb, Avery and Grimshaw, Jennica},
  editor = {Borthwick, Kate and Bradley, Linda and Thou{\"e}sny, Sylvie},
  month = dec,
  year = {2017},
  pages = {67-72},
  file = {/Users/kennylino/Documents/Articles/CardosoW/2017/Cardoso_2017_Can an interactive digital game help French learners improve their pronunciation.pdf},
  doi = {10.14705/rpnet.2017.eurocall2017.691}
}

@article{hardison2004,
  title = {Generalization of Computer Assisted Prosody Training: {{Quantitative}} and Qualitative Findings},
  abstract = {Two experiments investigated the effectiveness of computer-assisted prosody training, its generalization to novel sentences and segmental accuracy, and the relationship between prosodic and lexical information in long-term memory. Experiment 1, using a pretest-posttest design, provided native English-speaking learners of French with 3 weeks of training focused on prosody using a real-time computerized pitch display. Multiple exemplars produced by native speakers (NSs) of French and stored on hard disk provided training feedback. Learners' recorded pre- and posttest productions were presented to NSs for evaluation in two conditions: filtered (unintelligible segmental information) and unfiltered. Ratings using 7-point scales for the prosody and segmental accuracy of unfiltered samples revealed significant improvement in prosody with generalization to segmental production and novel sentences. Comparison of prosody ratings for filtered and unfiltered samples revealed some segmental influence on the pretest ratings of prosody. In Experiment 2, involving a memory recall task using filtered stimuli of reduced intelligibility, learners identified the exact lexical content of an average of 80\% of the training sentences based on prosodic cues consistent with exemplar-based learning models. Questionnaire responses indicated a greater awareness of the various aspects of speech and increased confidence in producing another language.},
  journal = {Language Learning},
  author = {Hardison, Debra M.},
  year = {2004},
  pages = {19},
  file = {/Users/kennylino/Documents/Articles/HardisonD/undefined/Hardison_GENERALIZATION OF COMPUTER-ASSISTED PROSODY TRAINING.pdf}
}

@article{hyde-simon2008,
  title = {{\emph{Phonology and }}{{{\emph{Second Language Acquisition}}}} - {{Edited}} by {{Jette G}}. {{Hansen Edwards}} and {{Mary L}}. {{Zampini}}},
  volume = {18},
  issn = {08026106, 14734192},
  doi = {10.1111/j.1473-4192.2008.00205.x},
  language = {en},
  number = {3},
  journal = {International Journal of Applied Linguistics},
  author = {Hyde-Simon, Caroline},
  month = nov,
  year = {2008},
  pages = {306-309},
  file = {/Users/kennylino/Documents/Articles/Hyde-SimonC/2008/Hyde-Simon_2008_iPhonology and Second Language Acquisition-i - Edited by Jette G.pdf}
}

@article{shroff2016,
  title = {{{GAMIFIED PEDAGOGY}}: {{EXAMINING HOW A PHONETICS APP COUPLED WITH EFFECTIVE PEDAGOGY CAN SUPPORT LEARNING}}},
  abstract = {Research has demonstrated that educational game-based apps may provide an approach to instruction in education that allows for greater learning outcomes. The focal context of this paper centres around the discussion of how gamified pedagogy supports learning. The first part of this paper will delve into the components of gaming, including the application of gamification to education and the methods by which digital game-based components such as scores and rewards are used to engage and motivate learners. The second part will focus on existing research on gaming pedagogy and the gaming elements of a phonetics app developed by the Resource Centre for Ubiquitous Learning and Integrated Pedagogy (ULIP) at Hong Kong Baptist University. The gamified pedagogical element of the app is designed to offer levels of challenge that motivate the players by making learning more exciting and rewarding. The game-based elements of the app promote active student involvement in learning, as the games are specifically designed to provide challenges and goals for players. Moreover, the need to capture and maintain the players' attention through visual experiences and audio designs is also an important element in the design of the app. When learners are engaged in a game-based app of this nature, they are not only reinforcing their cognitive skills, but they are also constantly drawing connections between images, text and sounds, thereby allowing students to learn and practise basic skills in order to master complex tasks.},
  author = {Shroff, Ronnie H and Keyes, Christopher J and Wee, Lian-Hee},
  year = {2016},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/ShroffR/2016/Shroff_2016_GAMIFIED PEDAGOGY.pdf}
}

@article{tejedor-garcia2016,
  title = {Improving {{L2 Production}} with a {{Gamified Computer}}-{{Assisted Pronunciation Training Tool}}, {{TipTopTalk}}!},
  abstract = {We present a foreign language (L2) pronunciation training serious game, TipTopTalk!, based on the minimal-pairs technique. We carried out a three-week test experiment where participants had to overcome several challenges including exposure, discrimination and production, while using Text-To-Speech (TTS) and Automatic Speech Recognition (ASR) systems in a mobile application. The quality of users' production is measured in order to assess their improvement. The application implements gamification resources with the aim of promoting continued practice. Preliminary results show that users with poorer initial performance levels make relatively more progress than the rest. However, it is desirable to include specific and individualized feedback in future versions so as to avoid the performance drop detected after the protracted use of the tool.},
  author = {Tejedor-Garc{\'\i}a, Cristian},
  year = {2016},
  keywords = {read},
  pages = {9},
  file = {/Users/kennylino/Documents/Articles/Tejedor-GarcC/undefined/Tejedor-Garc_Improving L2 Production with a Gamiﬁed Computer-Assisted Pronunciation Training.pdf}
}

@inproceedings{tejedor-garcia2017,
  title = {Evaluating the {{Efficiency}} of {{Synthetic Voice}} for {{Providing Corrective Feedback}} in a {{Pronunciation Training Tool Based}} on {{Minimal Pairs}}},
  doi = {10.21437/SLaTE.2017-5},
  abstract = {Feedback is an important concern in Computer-Assisted Pronunciation Training (CAPT), inasmuch as it bears on a system's capability to correct users' input and promote improved L2 pronunciation performance in the target language. In this paper, we test the use of synthetic voice as a corrective feedback resource. A group of students used a CAPT tool for carrying out a battery of minimal-pair discrimination-production tasks; to those who failed in production routines, the system offered the possibility of undergoing extra training by using synthetic voice as a model in a round of exposure exercises. Participants who made use of this resource significantly outperformed those who directly repeated the previously failed exercise. Results suggest that the Text-To-Speech systems offered by current operating systems (Android in our case) must be considered a relevant feedback resource in pronunciation training, especially when combined with efficient teaching methods.},
  language = {en},
  publisher = {{ISCA}},
  author = {Tejedor-Garc{\'\i}a, Cristian and Escudero, David and Gonz{\'a}lez-Ferreras, C{\'e}sar and C{\'a}mara-Arenas, Enrique and Carde{\~n}oso-Payo, Valent{\'\i}n},
  month = aug,
  year = {2017},
  pages = {25-29},
  file = {/Users/kennylino/Documents/Articles/Tejedor-GarcíaC/2017/Tejedor-García_2017_Evaluating the Efficiency of Synthetic Voice for Providing Corrective Feedback.pdf}
}

@article{eskenazi1998,
  title = {The {{Fluency Pronunciation Trainer}}},
  abstract = {In this article we describe the basis of the Fluency project for foreign language pronunciation training using automatic speech recognition. We describe the theoretical base, the interactive duration correction module, and our work toward adaptation to the way in which the user learns best. We show results in preliminary tests of the latter, and discuss future directions of the project.},
  author = {Eskenazi, Maxine and Hansma, Scott},
  year = {1998},
  keywords = {read},
  pages = {6},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/undefined/Eskenazi_The Fluency Pronunciation Trainer.pdf}
}

@incollection{lengeris2012,
  address = {Dordrecht},
  title = {Prosody and {{Second Language Teaching}}: {{Lessons}} from {{L2 Speech Perception}} and {{Production Research}}},
  volume = {15},
  isbn = {978-94-007-3882-9 978-94-007-3883-6},
  shorttitle = {Prosody and {{Second Language Teaching}}},
  booktitle = {Pragmatics and {{Prosody}} in {{English Language Teaching}}},
  publisher = {{Springer Netherlands}},
  author = {Lengeris, Angelos},
  editor = {Romero-Trillo, Jes{\'u}s},
  year = {2012},
  pages = {25-40},
  file = {/Users/kennylino/Documents/Articles/LengerisA/2012/Lengeris_2012_Prosody and Second Language Teaching.pdf},
  doi = {10.1007/978-94-007-3883-6_3}
}

@article{neri2002,
  title = {The {{Pedagogy}}-{{Technology Interface}} in {{Computer Assisted Pronunciation Training}}},
  volume = {15},
  issn = {0958-8221},
  doi = {10.1076/call.15.5.441.13473},
  abstract = {In this paper, we examine the relationship between pedagogy and technology in Computer Assisted Pronunciation Training (CAPT) courseware. First, we will analyse available literature on second language pronunciation teaching and learning in order to derive some general guidelines for effective training. Second, we will present an appraisal of various CAPT systems with a view to establishing whether they meet pedagogical requirements. In this respect, we will show that many commercial systems tend to prefer technological novelties to the detriment of pedagogical criteria that could benefit the learner more. While examining the limitations of today's technology, we will consider possible ways to deal with these shortcomings. Finally, we will combine the information thus gathered to suggest some recommendations for future CAPT.},
  number = {5},
  journal = {Computer Assisted Language Learning},
  author = {Neri, Ambra and Cucchiarini, Catia and Strik, Helmer and Boves, Lou},
  month = dec,
  year = {2002},
  keywords = {read},
  pages = {441-467},
  file = {/Users/kennylino/Documents/Articles/NeriA/2002/Neri_2002_The Pedagogy-Technology Interface in Computer Assisted Pronunciation Training.pdf}
}

@article{pellegrino2015,
  title = {Self-Imitation in Prosody Training: {{A}} Study on {{Japanese}} Learners of {{Italian}}},
  abstract = {The proficiency in a second language is fully attained only if students have learnt to modulate the rhythmic and prosodic parameters equivalent to those of the native speakers. This study is aimed to test the pedagogical effectiveness of the selfimitation technique for the purpose of developing a native-like prosodic competence. Seven intermediate Japanese learners of Italian (NNSs) and 2 native Italian speakers (NSs) were involved in a read speech activity. NSs and NNSs were asked to read and record two Italian sentences conveying three different pragmatic functions (granting, order, request). NNSs performed the task twice, before and after the self-imitation prosodic training. The items used for the training were obtained by transferring the suprasegmental features of the native speakers, used as donors, to the Japanese learners, considered as the receivers. During the training phase, Japanese learners mimic their utterances previously modified to match the prosody of the reference native speaker, and then recorded the new performance. Seventeen native Italian listeners rated pre- and post-training productions for pragmatic function and accentedness. The results indicate that selfimitation promoted an improvement in learners' performances in terms of communicative effectiveness. Conversely, average rate of accentedness does not change significantly before and after training.},
  author = {Pellegrino, Elisa and Vigliano, Debora},
  year = {2015},
  pages = {5},
  file = {/Users/kennylino/Documents/Articles/PellegrinoE/2015/Pellegrino_2015_Self-imitation in prosody training.pdf}
}

@article{darcy2012,
  title = {Bringing Pronunciation Instruction Back into the Classroom: An {{ESL}} Teachers' Pronunciation "Toolbox"},
  author = {Darcy, Isabelle and Ewert, Doreen and Lidster, Ryan},
  year = {2012},
  keywords = {read},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/DarcyI/2012/Darcy_2012_BRINGING PRONUNCIATION INSTRUCTION BACK INTO THE CLASSROOM.pdf}
}

@article{gangireddy2015,
  title = {Prosodically-Enhanced {{Recurrent Neural Network Language Models}}},
  abstract = {Recurrent neural network language models have been shown to consistently reduce the word error rates (WERs) of large vocabulary speech recognition tasks. In this work we propose to enhance the RNNLMs with prosodic features computed using the context of the current word. Since it is plausible to compute the prosody features at the word and syllable level we have trained the models on prosody features computed at both these levels. To investigate the effectiveness of proposed models we report perplexity and WER for two speech recognition tasks, Switchboard and TED. We observed substantial improvements in perplexity and small improvements in WER.},
  author = {Gangireddy, Siva Reddy and Renals, Steve and Nankaku, Yoshihiko and Lee, Akinobu},
  year = {2015},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/WUKRJSGR/Gangireddy et al. - Prosodically-enhanced Recurrent Neural Network Lan.pdf}
}

@article{gauthier2009,
  title = {Learning {{Prosodic Focus}} from {{Continuous Speech Input}}:{{A Neural Network Exploration}}},
  volume = {5},
  issn = {1547-5441, 1547-3341},
  shorttitle = {Learning {{Prosodic Focus}} from {{Continuous Speech Input}}},
  doi = {10.1080/15475440802698524},
  language = {en},
  number = {2},
  journal = {Language Learning and Development},
  author = {Gauthier, Bruno and Shi, Rushen and Xu, Yi},
  month = apr,
  year = {2009},
  pages = {94-114},
  file = {/Users/kennylino/Zotero/storage/NZ57BXXN/Gauthier et al. - 2009 - Learning Prosodic Focus from Continuous Speech Inp.pdf}
}

@article{bernardy2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03952},
  primaryClass = {cs},
  title = {Modelling Prosodic Structure Using {{Artificial Neural Networks}}},
  abstract = {The ability to accurately perceive whether a speaker is asking a question or is making a statement is crucial for any successful interaction. However, learning and classifying tonal patterns has been a challenging task for automatic speech recognition and for models of tonal representation, as tonal contours are characterized by significant variation. This paper provides a classification model of Cypriot Greek questions and statements. We evaluate two state-of-the-art network architectures: a Long Short-Term Memory (LSTM) network and a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the classification task and exhibited an excellent performance with 95\% classification accuracy.},
  journal = {arXiv:1706.03952 [cs]},
  author = {Bernardy, Jean-Philippe and Themistocleous, Charalambos},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computation and Language,read},
  file = {/Users/kennylino/Documents/Articles/BernardyJ/2017/Bernardy_2017_Modelling prosodic structure using Artificial Neural Networks.pdf;/Users/kennylino/Zotero/storage/3TDZUD4J/1706.html}
}

@phdthesis{vainio2001,
  address = {Helsinki},
  title = {Artificial Neural Network Based Prosody Models for {{Finnish}} Text-to-Speech Synthesis},
  language = {English},
  school = {University of Helsinki},
  author = {Vainio, Martti},
  year = {2001},
  file = {/Users/kennylino/Zotero/storage/VT9NT5CP/Vainio - 2001 - Artificial neural network based prosody models for.pdf},
  note = {OCLC: 58373280}
}

@article{han2014,
  title = {Speech {{Emotion Recognition Using Deep Neural Network}} and {{Extreme Learning Machine}}},
  abstract = {Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We first produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterancelevel features are then fed into an extreme learning machine (ELM), a special simple and efficient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20\% relative accuracy improvement compared to the stateof-the-art approaches.},
  author = {Han, Kun and Yu, Dong and Tashev, Ivan},
  year = {2014},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/8SRWUCP5/Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf}
}

@inproceedings{fu2015,
  title = {Integrating Prosodic Information into Recurrent Neural Network Language Model for Speech Recognition},
  isbn = {978-988-14768-0-7},
  doi = {10.1109/APSIPA.2015.7415462},
  abstract = {Prosody is a kind of cues that are critical to human speech perception and comprehension, so it is plausible to integrate prosodic information into machine speech recognition. However, as a result of the supra-segmental nature, it is hard to integrate prosodic information with conventional acoustic features. Recently, RNNLMs have shown to be the state-of-theart language model in many tasks. We thus attempt to integrate prosodic information into RNNLMs for improving speech recognition performance based on rescoring strategy. Firstly, three word-level prosodic features are extracted from speech and then passed to RNNLMs separately. Therefore RNNLMs predict the next word based on prosodic features and word history. Experiments conducted on LibriSpeech Corpus show that the word error rate decreases from 8.07\% to 7.96\%. Secondly, prosodic information is combined on feature-level and modellevel for further improvements and word error rate decreases 4.71\% relatively.},
  language = {en},
  publisher = {{IEEE}},
  author = {Fu, Tong and Han, Yang and Li, Xiangang and Liu, Yi and Wu, Xihong},
  month = dec,
  year = {2015},
  pages = {1194-1197},
  file = {/Users/kennylino/Zotero/storage/C4KRBWHJ/Fu et al. - 2015 - Integrating prosodic information into recurrent ne.pdf}
}

@article{mao2014,
  title = {Learning {{Salient Features}} for {{Speech Emotion Recognition Using Convolutional Neural Networks}}},
  volume = {16},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2014.2360798},
  abstract = {As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on finding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the first stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.},
  language = {en},
  number = {8},
  journal = {IEEE Transactions on Multimedia},
  author = {Mao, Qirong and Dong, Ming and Huang, Zhengwei and Zhan, Yongzhao},
  month = dec,
  year = {2014},
  pages = {2203-2213},
  file = {/Users/kennylino/Zotero/storage/DRJXYBPX/Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf}
}

@inproceedings{stuhlsatz2011,
  title = {Deep Neural Networks for Acoustic Emotion Recognition: {{Raising}} the Benchmarks},
  isbn = {978-1-4577-0538-0},
  shorttitle = {Deep Neural Networks for Acoustic Emotion Recognition},
  doi = {10.1109/ICASSP.2011.5947651},
  abstract = {Deep Neural Networks (DNNs) denote multilayer artificial neural networks with more than one hidden layer and millions of free parameters. We propose a Generalized Discriminant Analysis (GerDA) based on DNNs to learn discriminative features of low dimension optimized with respect to a fast classification from a large set of acoustic features for emotion recognition. On nine frequently used emotional speech corpora, we compare the performance of GerDA features and their subsequent linear classification with previously reported benchmarks obtained using the same set of acoustic features classified by Support Vector Machines (SVMs). Our results impressively show that low-dimensional GerDA features capture hidden information from the acoustic features leading to a significantly raised unweighted average recall and considerably raised weighted average recall.},
  language = {en},
  publisher = {{IEEE}},
  author = {Stuhlsatz, Andre and Meyer, Christine and Eyben, Florian and Zielke, Thomas and Meier, Gunter and Schuller, Bjorn},
  month = may,
  year = {2011},
  pages = {5688-5691},
  file = {/Users/kennylino/Zotero/storage/M6MMQ2YC/Stuhlsatz et al. - 2011 - Deep neural networks for acoustic emotion recognit.pdf}
}

@inproceedings{luengo2005,
  title = {Automatic {{Emotion Recognition}} Using {{Prosodic Parameters}}},
  abstract = {This paper presents the experiments made to automatically identify emotion in an emotional speech database for Basque. Three different classifiers have been built: one using spectral features and GMM, other with prosodic features and SVM and the last one with prosodic features and GMM. 86 prosodic features were calculated and then an algorithm to select the most relevant ones was applied. The first classifier gives the best result with a 98.4 \% accuracy when using 512 mixtures, but the classifier built with the best 6 prosodic features achieves an accuracy of 92.3 \% in spite of its simplicity, showing that prosodic information is very useful to identify emotions. 1.},
  booktitle = {In {{Proc}}. of {{INTERSPEECH}}},
  author = {Luengo, Iker and Navas, Eva and Hern{\'a}ez, Inmaculada and S{\'a}nchez, Jon},
  year = {2005},
  pages = {493--496},
  file = {/Users/kennylino/Documents/Articles/LuengoI/2005/Luengo_2005_Automatic Emotion Recognition using Prosodic Parameters.pdf;/Users/kennylino/Zotero/storage/R3G6QY9Y/summary.html}
}

@inproceedings{jorgensen2015,
  address = {Beijing, China},
  title = {Challenges of Studying and Processing Dialects in Social Media},
  booktitle = {Proceedings of the {{Workshop}} on {{Noisy User}}-Generated {{Text}}},
  publisher = {{Association for Computational Linguistics}},
  author = {J{\o}rgensen, Anna and Hovy, Dirk and S{\o}gaard, Anders},
  month = jul,
  year = {2015},
  pages = {9--18},
  file = {/Users/kennylino/Documents/Articles/JørgensenA/2015/Jørgensen_2015_Challenges of studying and processing dialects in social media.pdf}
}

@inproceedings{sarawgi2011,
  address = {Portland, Oregon, USA},
  title = {Gender {{Attribution}}: {{Tracing Stylometric Evidence Beyond Topic}} and {{Genre}}},
  shorttitle = {Gender {{Attribution}}},
  booktitle = {Proceedings of the {{Fifteenth Conference}} on {{Computational Natural Language Learning}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Sarawgi, Ruchita and Gajulapalli, Kailash and Choi, Yejin},
  month = jun,
  year = {2011},
  pages = {78--86},
  file = {/Users/kennylino/Documents/Articles/SarawgiR/2011/Sarawgi_2011_Gender Attribution.pdf}
}

@article{skerry-ryan2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09047},
  primaryClass = {cs, eess},
  title = {Towards {{End}}-to-{{End Prosody Transfer}} for {{Expressive Speech Synthesis}} with {{Tacotron}}},
  abstract = {We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.},
  journal = {arXiv:1803.09047 [cs, eess]},
  author = {Skerry-Ryan, R. J. and Battenberg, Eric and Xiao, Ying and Wang, Yuxuan and Stanton, Daisy and Shor, Joel and Weiss, Ron J. and Clark, Rob and Saurous, Rif A.},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/Skerry-RyanR/2018/Skerry-Ryan_2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with.pdf;/Users/kennylino/Zotero/storage/TVK5RWX4/1803.html}
}

@article{vajjala2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09103},
  primaryClass = {cs},
  title = {Machine {{Learning}} and {{Applied Linguistics}}},
  abstract = {This entry introduces the topic of machine learning and provides an overview of its relevance for applied linguistics and language learning. The discussion will focus on giving an introduction to the methods and applications of machine learning in applied linguistics, and will provide references for further study.},
  journal = {arXiv:1803.09103 [cs]},
  author = {Vajjala, Sowmya},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/kennylino/Documents/Articles/VajjalaS/2018/Vajjala_2018_Machine Learning and Applied Linguistics.pdf;/Users/kennylino/Zotero/storage/D9ZVK59G/1803.html}
}

@article{wang2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09017},
  primaryClass = {cs, eess},
  title = {Style {{Tokens}}: {{Unsupervised Style Modeling}}, {{Control}} and {{Transfer}} in {{End}}-to-{{End Speech Synthesis}}},
  shorttitle = {Style {{Tokens}}},
  abstract = {In this work, we propose "global style tokens" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable "labels" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.},
  journal = {arXiv:1803.09017 [cs, eess]},
  author = {Wang, Yuxuan and Stanton, Daisy and Zhang, Yu and Skerry-Ryan, R. J. and Battenberg, Eric and Shor, Joel and Xiao, Ying and Ren, Fei and Jia, Ye and Saurous, Rif A.},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/WangY/2018/Wang_2018_Style Tokens.pdf;/Users/kennylino/Zotero/storage/4ZQQ8KCN/1803.html}
}

@article{wang2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00520},
  primaryClass = {cs},
  title = {Uncovering {{Latent Style Factors}} for {{Expressive Speech Synthesis}}},
  abstract = {Prosodic modeling is a core problem in speech synthesis. The key challenge is producing desirable prosody from textual input containing only phonetic information. In this preliminary study, we introduce the concept of "style tokens" in Tacotron, a recently proposed end-to-end neural speech synthesis model. Using style tokens, we aim to extract independent prosodic styles from training data. We show that without annotation data or an explicit supervision signal, our approach can automatically learn a variety of prosodic variations in a purely data-driven way. Importantly, each style token corresponds to a fixed style factor regardless of the given text sequence. As a result, we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way.},
  journal = {arXiv:1711.00520 [cs]},
  author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Xiao, Ying and Stanton, Daisy and Shor, Joel and Battenberg, Eric and Clark, Rob and Saurous, Rif A.},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound},
  file = {/Users/kennylino/Documents/Articles/WangY/2017/Wang_2017_Uncovering Latent Style Factors for Expressive Speech Synthesis.pdf;/Users/kennylino/Zotero/storage/ZIL9IE6Y/1711.html}
}

@article{wang2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10135},
  primaryClass = {cs},
  title = {Tacotron: {{Towards End}}-to-{{End Speech Synthesis}}},
  shorttitle = {Tacotron},
  abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given $<$text, audio$>$ pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
  journal = {arXiv:1703.10135 [cs]},
  author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound},
  file = {/Users/kennylino/Documents/Articles/WangY/2017/Wang_2017_Tacotron.pdf;/Users/kennylino/Zotero/storage/UT9FA57I/1703.html}
}

@incollection{sutskever2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {3104--3112},
  file = {/Users/kennylino/Zotero/storage/XXIJ3UMZ/5346-sequence-to-sequence-learning-with-neural.html}
}

@article{arik2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06006},
  primaryClass = {cs, eess},
  title = {Neural {{Voice Cloning}} with a {{Few Samples}}},
  abstract = {Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.},
  journal = {arXiv:1802.06006 [cs, eess]},
  author = {Arik, Sercan O. and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/ArikS/2018/Arik_2018_Neural Voice Cloning with a Few Samples2.pdf;/Users/kennylino/Zotero/storage/XMMJPJG3/1802.html}
}

@article{nguyen2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.01809},
  title = {High Quality Voice Conversion Using Prosodic and High-Resolution Spectral Features},
  volume = {75},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-015-3039-x},
  abstract = {Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor-made for voice conversion and leverages on autoencoder to capture the generic spectral shape of source speech. Additionally, our framework uses segmental DNN models to capture the evolution of the prosodic features over time. To reconstruct the converted speech, the spectral feature produced by the DNN model is combined with the three prosodic features produced by the DNN segmental models. Our experimental results show that the application of both prosodic and high-resolution spectral features leads to quality converted speech as measured by objective evaluation and subjective listening tests.},
  number = {9},
  journal = {Multimedia Tools and Applications},
  author = {Nguyen, Hy Quy and Lee, Siu Wa and Tian, Xiaohai and Dong, Minghui and Chng, Eng Siong},
  month = may,
  year = {2016},
  keywords = {Computer Science - Sound},
  pages = {5265-5285},
  file = {/Users/kennylino/Documents/Articles/NguyenH/2016/Nguyen_2016_High quality voice conversion using prosodic and high-resolution spectral.pdf;/Users/kennylino/Zotero/storage/E4MU3IYC/1512.html}
}

@article{chorowski2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.08363},
  primaryClass = {cs, eess, stat},
  title = {On {{Using Backpropagation}} for {{Speech Texture Generation}} and {{Voice Conversion}}},
  abstract = {Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice.},
  journal = {arXiv:1712.08363 [cs, eess, stat]},
  author = {Chorowski, Jan and Weiss, Ron J. and Saurous, Rif A. and Bengio, Samy},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/kennylino/Documents/Articles/ChorowskiJ/2017/Chorowski_2017_On Using Backpropagation for Speech Texture Generation and Voice Conversion.pdf;/Users/kennylino/Zotero/storage/HYAFTY3Z/1712.html}
}

@article{mohammadi2017,
  title = {An Overview of Voice Conversion Systems},
  volume = {88},
  issn = {01676393},
  doi = {10.1016/j.specom.2017.01.008},
  abstract = {Voice transformation (VT) aims to change one or more aspects of a speech signal while preserving linguistic information. A subset of VT, Voice conversion (VC) specifically aims to change a source speaker's speech in such a way that the generated output is perceived as a sentence uttered by a target speaker. Despite many years of research, VC systems still exhibit deficiencies in accurately mimicking a target speaker spectrally and prosodically, and simultaneously maintaining high speech quality. In this work we provide an overview of real-world applications, extensively study existing systems proposed in the literature, and discuss remaining challenges.},
  language = {en},
  journal = {Speech Communication},
  author = {Mohammadi, Seyed Hamidreza and Kain, Alexander},
  month = apr,
  year = {2017},
  pages = {65-82},
  file = {/Users/kennylino/Zotero/storage/TXB6AX59/Mohammadi and Kain - 2017 - An overview of voice conversion systems.pdf}
}

@inproceedings{luo2016,
  title = {Emotional Voice Conversion Using Deep Neural Networks with {{MCC}} and {{F0}} Features},
  doi = {10.1109/ICIS.2016.7550889},
  abstract = {An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.},
  booktitle = {2016 {{IEEE}}/{{ACIS}} 15th {{International Conference}} on {{Computer}} and {{Information Science}} ({{ICIS}})},
  author = {Luo, Z. and Takiguchi, T. and Ariki, Y.},
  month = jun,
  year = {2016},
  keywords = {artificial neural network,Artificial neural networks,belief networks,cepstral analysis,DBN,deep belief network,deep neural network,emotional voice conversion,F0 feature,feature extraction,Feature extraction,MCC,mel cepstral coefficient,Mel frequency cepstral coefficient,neural nets,NN,prosody signal,spectrum feature training,Speech,speech processing,Training,Transforms},
  pages = {1-5},
  file = {/Users/kennylino/Zotero/storage/BNILSKCY/7550889.html}
}

@inproceedings{aryal2014,
  title = {Can Voice Conversion Be Used to Reduce Non-Native Accents?},
  isbn = {978-1-4799-2893-4},
  doi = {10.1109/ICASSP.2014.6855134},
  abstract = {Voice-conversion (VC) techniques aim to transform utterances from a source speaker to sound as if they had been produced by a target speaker. This includes not only organic properties (i.e., voice quality) but also linguistic cues (i.e., regional accents) of the target speaker. For this reason, VC is generally ill-suited for accent-conversion (AC) purposes, where the goal is to capture the voice quality of the target speaker but the regional accent of the source speaker. In this paper, we propose a modification of the conventional training process for VC that allows it to perform as an AC transform. The approach consists of pairing source and target vectors based not on their ordering within a parallel corpus, as is commonly done in VC, but based on their linguistic similarity. We validate the AC approach on a corpus containing native-accented and Spanish-accented utterances, and compare it against conventional VC through a series of perceptual listening tests. We also analyze the extent to which phonological differences between the two languages (Spanish and American English) help predict the relative performance of the two methods.},
  language = {en},
  publisher = {{IEEE}},
  author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
  month = may,
  year = {2014},
  pages = {7879-7883},
  file = {/Users/kennylino/Zotero/storage/LZS5YPC9/Aryal and Gutierrez-Osuna - 2014 - Can voice conversion be used to reduce non-native .pdf}
}

@article{felps2010,
  title = {Developing Objective Measures of Foreign-Accent Conversion},
  abstract = {Abstract\textemdash{}Various methods have recently appeared to transform foreign-accented speech into its native-accented counterpart. Eval-uation of these accent conversion methods requires extensive lis-tening tests across a number of perceptual dimensions. This article presents three objective measures that may be used to assess the acoustic quality, degree of foreign accent, and speaker identity of accent-converted utterances. Accent conversion generates novel ut-terances: those of a foreign speaker with a native accent. There-fore, the acoustic quality in accent conversion cannot be evaluated with conventional measures of spectral distortion, which assume that a clean recording of the speech signal is available for compar-ison. Here we evaluate a single-ended measure of speech quality, ITU-T recommendation P.563 for narrow-band telephony. We also propose a measure of foreign accent that exploits a weakness of automatic speech recognizers: their sensitivity to foreign accents. Namely, we use phoneme-level match scores given by the HTK rec-ognizer trained on a large number of English American speakers to obtain a measure of native accent. Finally, we propose a measure of speaker identity that projects acoustic vectors (e.g., Mel cep-stral, F0) onto the linear discriminant that maximizes separability for a given pair of source and target speakers. The three measures are evaluated on a corpus of accent-converted utterances that had been previously rated through perceptual tests. Our results show that the three measures have a high degree of correlation with their corresponding subjective ratings, suggesting that they may be used to accelerate the development of foreign-accent conversion tools. Applications of these measures in the context of computer assisted pronunciation training and voice conversion are also discussed. Index Terms\textemdash{}Accent conversion, foreign accent recognition, speaker recognition, voice conversion. I.},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Felps, Daniel and Member, Student and Gutierrez-osuna, Ricardo and Member, Senior},
  year = {2010},
  pages = {1030--1040},
  file = {/Users/kennylino/Documents/Articles/FelpsD/2010/Felps_2010_Developing objective measures of foreign-accent conversion.pdf;/Users/kennylino/Zotero/storage/LXQSAWW2/summary.html}
}

@article{aryal2010,
  title = {Foreign {{Accent Conversion Through Voice Morphing}}},
  abstract = {We present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion. The technique is validated on parallel recordings from two ARCTIC speakers using both objective and subjective measures of acoustic quality, speaker identity and foreign accent.},
  language = {en},
  author = {Aryal, Sandesh and Felps, Daniel and Gutierrez-Osuna, Ricardo},
  year = {2010},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/JCYBS2C4/Aryal et al. - Foreign Accent Conversion Through Voice Morphing.pdf}
}

@inproceedings{aryal2014a,
  title = {Accent Conversion through Cross-Speaker Articulatory Synthesis},
  isbn = {978-1-4799-2893-4},
  doi = {10.1109/ICASSP.2014.6855097},
  abstract = {Accent conversion (AC) seeks to transform second-language (L2) utterances to appear as if produced with a native (L1) accent. In the acoustic domain, AC is difficult due to the complex interaction between linguistic content and voice quality. Alternatively, AC can be performed in the articulatory domain by building a mapping from L2 articulators to L2 acoustics, and then driving the model with L1 articulators. However, collecting articulatory data for each L2 learner is impractical. Here we propose an approach that avoids this expensive step. Our method builds a cross-speaker forward mapping (CSFM) to generate L2 acoustic observations directly from L1 articulatory trajectories. We evaluated the CSFM against a baseline articulatory synthesizer trained with L2 articulators. Subjective listening tests show that both methods perform comparably in terms of accent reduction and ability to preserve the voice quality of the L2 speaker, with only a small impact in acoustic quality.},
  language = {en},
  publisher = {{IEEE}},
  author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
  month = may,
  year = {2014},
  pages = {7694-7698},
  file = {/Users/kennylino/Zotero/storage/TDJ33NSU/Aryal and Gutierrez-Osuna - 2014 - Accent conversion through cross-speaker articulato.pdf}
}

@article{gutierrez-osuna2013,
  title = {Foreign Accent Conversion through Voice Morphing},
  abstract = {We present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion. The technique is evaluated on parallel recordings from two ARCTIC speakers using objective measures of acoustic quality, speaker identity and foreign accent that have been recently shown to correlate with perceptual results from listening tests.},
  language = {en},
  author = {Gutierrez-Osuna, Ricardo and Felps, Daniel},
  year = {2013},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/TYI5IZIZ/Gutierrez-Osuna and Felps - Foreign accent conversion through voice morphing.pdf}
}

@inproceedings{toda2016,
  title = {The {{Voice Conversion Challenge}} 2016},
  doi = {10.21437/Interspeech.2016-1066},
  abstract = {This paper describes the Voice Conversion Challenge 2016 devised by the authors to better understand different voice conversion (VC) techniques by comparing their performance on a common dataset. The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. These samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This paper summarizes the design of the challenge, its result, and a future plan to share views about unsolved problems and challenges faced by the current VC techniques.},
  language = {en},
  author = {Toda, Tomoki and Chen, Ling-Hui and Saito, Daisuke and Villavicencio, Fernando and Wester, Mirjam and Wu, Zhizheng and Yamagishi, Junichi},
  month = sep,
  year = {2016},
  pages = {1632-1636},
  file = {/Users/kennylino/Zotero/storage/PFYB59NR/Toda et al. - 2016 - The Voice Conversion Challenge 2016.pdf}
}

@article{luo2017,
  title = {Emotional Voice Conversion Using Neural Networks with Arbitrary Scales {{F0}} Based on Wavelet Transform},
  volume = {2017},
  issn = {1687-4722},
  doi = {10.1186/s13636-017-0116-2},
  abstract = {An artificial neural network is an important model for training features of voice conversion (VC) tasks. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as Mel Cepstral Coefficients (MCC), which represent the spectrum features. However, a simple representation of fundamental frequency (F0) is not enough for NNs to deal with emotional voice VC. This is because the time sequence of F0 for an emotional voice changes drastically. Therefore, in our previous method, we used the continuous wavelet transform (CWT) to decompose F0 into 30 discrete scales, each separated by one third of an octave, which can be trained by NNs for prosody modeling in emotional VC. In this study, we propose the arbitrary scales CWT (AS-CWT) method to systematically capture F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, the proposed method uses deep belief networks (DBNs) to pre-train the NNs that then convert spectral features. By utilizing these approaches, the proposed method can change the spectrum and the F0 for an emotional voice simultaneously as well as outperform other state-of-the-art methods in terms of emotional VC.},
  language = {en},
  number = {1},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  author = {Luo, Zhaojie and Chen, Jinhui and Takiguchi, Tetsuya and Ariki, Yasuo},
  month = dec,
  year = {2017},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/LuoZ/2017/Luo_2017_Emotional voice conversion using neural networks with arbitrary scales F0 based.pdf;/Users/kennylino/Zotero/storage/RR4JHYB6/s13636-017-0116-2.html}
}

@article{lorenzo-trueba2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.04262},
  primaryClass = {cs, eess, stat},
  title = {The {{Voice Conversion Challenge}} 2018: {{Promoting Development}} of {{Parallel}} and {{Nonparallel Methods}}},
  shorttitle = {The {{Voice Conversion Challenge}} 2018},
  abstract = {We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.},
  journal = {arXiv:1804.04262 [cs, eess, stat]},
  author = {Lorenzo-Trueba, Jaime and Yamagishi, Junichi and Toda, Tomoki and Saito, Daisuke and Villavicencio, Fernando and Kinnunen, Tomi and Ling, Zhenhua},
  month = apr,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/kennylino/Documents/Articles/Lorenzo-TruebaJ/2018/Lorenzo-Trueba_2018_The Voice Conversion Challenge 2018.pdf;/Users/kennylino/Zotero/storage/V4PHRZST/1804.html}
}

@article{azarov2013,
  title = {Real-{{Time Voice Conversion Using Artificial Neural Networks}} with {{Rectified Linear Units}}},
  abstract = {This paper presents an approach to parametric voice conversion that can be used in real-time entertainment applications. The approach is based on spectral mapping using an artificial neural network (ANN) with rectified linear units (ReLU). To overcome the oversmoothing problem a special network configuration is proposed that utilizes temporal states of the speaker. The speech is represented using the harmonic plus noise model. The parameters of the model are estimated using instantaneous harmonic parameters. Using objective and subjective measures the proposed voice conversion technique is compared to the main alternative approaches.},
  language = {en},
  author = {Azarov, Elias and Vashkevich, Maxim and Likhachov, Denis and Petrovsky, Alexander},
  year = {2013},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/J8ZF22C2/Azarov et al. - Real-Time Voice Conversion Using Artificial Neural.pdf}
}

@inproceedings{sun2015,
  title = {Voice Conversion Using Deep {{Bidirectional Long Short}}-{{Term Memory}} Based {{Recurrent Neural Networks}}},
  doi = {10.1109/ICASSP.2015.7178896},
  abstract = {This paper investigates the use of Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM-RNNs) for voice conversion. Temporal correlations across speech frames are not directly modeled in frame-based methods using conventional Deep Neural Networks (DNNs), which results in a limited quality of the converted speech. To improve the naturalness and continuity of the speech output in voice conversion, we propose a sequence-based conversion method using DBLSTM-RNNs to model not only the frame-wised relationship between the source and the target voice, but also the long-range context-dependencies in the acoustic trajectory. Experiments show that DBLSTM-RNNs outperform DNNs where Mean Opinion Scores are 3.2 and 2.3 respectively. Also, DBLSTM-RNNs without dynamic features have better performance than DNNs with dynamic features.},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sun, L. and Kang, S. and Li, K. and Meng, H.},
  month = apr,
  year = {2015},
  keywords = {Speech,speech processing,Training,acoustic trajectory,Acoustics,bidirectional long short-term memory,Context,DBLSTM-RNNs,deep bidirectional long short-term memory based recurrent neural networks,DNNs,dynamic features,frame-based methods,Logic gates,long-range context-dependency,mean opinion scores,recurrent neural nets,recurrent neural networks,Recurrent neural networks,sequence-based conversion method,speech frames,temporal correlations,voice conversion},
  pages = {4869-4873},
  file = {/Users/kennylino/Documents/Articles/SunL/2015/Sun_2015_Voice conversion using deep Bidirectional Long Short-Term Memory based.pdf;/Users/kennylino/Zotero/storage/IBVUDJUQ/7178896.html}
}

@inproceedings{desai2009,
  title = {Voice Conversion Using {{Artificial Neural Networks}}},
  doi = {10.1109/ICASSP.2009.4960478},
  abstract = {In this paper, we propose to use artificial neural networks (ANN) for voice conversion. We have exploited the mapping abilities of ANN to perform mapping of spectral features of a source speaker to that of a target speaker. A comparative study of voice conversion using ANN and the state-of-the-art Gaussian mixture model (GMM) is conducted. The results of voice conversion evaluated using subjective and objective measures confirm that ANNs perform better transformation than GMMs and the quality of the transformed speech is intelligible and has the characteristics of the target speaker.},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Desai, S. and Raghavendra, E. V. and Yegnanarayana, B. and Black, A. W. and Prahallad, K.},
  month = apr,
  year = {2009},
  keywords = {Artificial neural networks,neural nets,speech processing,voice conversion,ANN,artificial neural networks,Artificial Neural Networks,Books,Data mining,Databases,Filters,Frequency estimation,Gaussian mixture model,Gaussian Mixture Model,Gaussian processes,Loudspeakers,source speaker,spectral analysis,spectral feature mapping,speech intelligibility,Speech synthesis,target speaker,Training data,Vectors,Voice conversion},
  pages = {3893-3896},
  file = {/Users/kennylino/Documents/Articles/DesaiS/2009/Desai_2009_Voice conversion using Artificial Neural Networks.pdf;/Users/kennylino/Zotero/storage/MWIINPND/4960478.html}
}

@article{chen2014,
  title = {Voice {{Conversion Using Deep Neural Networks}} with {{Layer}}-Wise {{Generative Training}}},
  volume = {22},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2014.2353991},
  abstract = {This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.},
  number = {12},
  journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  author = {Chen, Ling-Hui and Ling, Zhen-Hua and Liu, Li-Juan and Dai, Li-Rong},
  month = dec,
  year = {2014},
  keywords = {deep neural network,voice conversion,Gaussian mixture model,bidirectional associative memory,restricted Boltzmann machine,spectral envelope conversion},
  pages = {1859--1872},
  file = {/Users/kennylino/Documents/Articles/ChenL/2014/Chen_2014_Voice Conversion Using Deep Neural Networks with Layer-wise Generative Training.pdf}
}

@inproceedings{mohammadi2014,
  title = {Voice Conversion Using Deep Neural Networks with Speaker-Independent Pre-Training},
  doi = {10.1109/SLT.2014.7078543},
  abstract = {In this study, we trained a deep autoencoder to build compact representations of short-term spectra of multiple speakers. Using this compact representation as mapping features, we then trained an artificial neural network to predict target voice features from source voice features. Finally, we constructed a deep neural network from the trained deep autoencoder and artificial neural network weights, which were then fine-tuned using back-propagation. We compared the proposed method to existing methods using Gaussian mixture models and frame-selection. We evaluated the methods objectively, and also conducted perceptual experiments to measure both the conversion accuracy and speech quality of selected systems. The results showed that, for 70 training sentences, frame-selection performed best, regarding both accuracy and quality. When using only two training sentences, the pre-trained deep neural network performed best, regarding both accuracy and quality.},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Mohammadi, S. H. and Kain, A.},
  month = dec,
  year = {2014},
  keywords = {Artificial neural networks,deep neural network,neural nets,Speech,Training,voice conversion,Gaussian processes,Accuracy,artificial neural network training,autoencoder,back-propagation,backpropagation,Conferences,conversion accuracy,deep autoencoder,frame-selection,Gaussian mixture models,mapping features,mixture models,pre-training,signal representation,source voice features,speaker recognition,speaker short-term spectra compact representation,speaker-independent pretraining,Speech processing,speech quality,target voice feature prediction},
  pages = {19-23},
  file = {/Users/kennylino/Documents/Articles/MohammadiS/2014/Mohammadi_2014_Voice conversion using deep neural networks with speaker-independent.pdf;/Users/kennylino/Zotero/storage/VT7UYVHD/7078543.html}
}

@article{lenneberg1967,
  title = {The {{Biological Foundations}} of {{Language}}},
  volume = {2},
  issn = {2154-8331},
  doi = {10.1080/21548331.1967.11707799},
  abstract = {The coming of language occurs at about the same age in every healthy child throughout the world, strongly supporting the concept that genetically determined processes of maturation, rather than environmental influences, underlie capacity for speech and verbal understanding. Dr. Lenneberg points out the implications of this concept for the therapeutic and educational approach to children with hearing or speech deficits.},
  number = {12},
  journal = {Hospital Practice},
  author = {Lenneberg, Eric H.},
  month = dec,
  year = {1967},
  pages = {59-67},
  file = {/Users/kennylino/Zotero/storage/IFTYRDQN/21548331.1967.html}
}

@book{scovel1988,
  title = {A Time to Speak: {{A}} Psycholinguistic Inquiry into the Critical Period for Human Speech},
  author = {Scovel, Thomas},
  year = {1988}
}

@incollection{bongaerts1995,
  title = {Can {{Late Starters Attain}} a {{Native Accent}} in a {{Foreign Language}}? {{A Test}} of the {{Critical Period Hypothesis}}},
  author = {Bongaerts, Theo and Planken, Brigitte and Schils, Erik},
  year = {1995}
}

@article{munromurrayj.2006,
  title = {Foreign {{Accent}}, {{Comprehensibility}}, and {{Intelligibility}} in the {{Speech}} of {{Second Language Learners}}},
  volume = {45},
  issn = {0023-8333},
  doi = {10.1111/j.1467-1770.1995.tb00963.x},
  abstract = {This study examines the interrelationships among accentedness, perceived comprehensibility, and intelligi bility in the speech of L2 learners. Eighteen native speak ers (NSs) of English listened to excerpts of extemporaneous English speech produced by 10 Mandarin NSs and two English NSs. We asked the listeners to transcribe the utterances in standard orthography and to rate them for degree of foreign?accentedness and comprehensibility on 9? point scales. We assigned the transcriptions intelligibility scores on the basis of exact word matches. Although the utterances tended to be highly intelligible and highly rated for comprehensibility, the accent judgment scores ranged widely, with a noteworthy proportion of scores at the ?heavily?accented? end of the scale. We calculated Pearson correlations for each listener's intelligibility, accentedness, and comprehensibility scores and the phonetic, phonemic, and grammatical errors in the stimuli, as well as goodness of intonation ratings. Most listeners showed significant correlations between accentedness and errors, fewer lis teners showed correlations between accentedness and per ceived comprehensibility, and fewer still showed a rela tionship between accentedness and intelligibility. The findings suggest that although strength of foreign accent is correlated with perceived comprehensibility and intelligibility, a strong foreign accent does not necessarily reduce the comprehensibility or intelligibility of L2 speech.},
  number = {1},
  journal = {Language Learning},
  author = {{Munro Murray J.} and {Derwing Tracey M.}},
  month = oct,
  year = {2006},
  pages = {73-97},
  file = {/Users/kennylino/Zotero/storage/DMQESIDM/j.1467-1770.1995.tb00963.html}
}

@article{wu2010,
  title = {Text-{{Independent F0 Transformation}} with {{Non}}-{{Parallel Data}} for {{Voice Conversion}}},
  abstract = {In voice conversion, frame-level mean and variance normalization is typically used for fundamental frequency (F0) transformation, which is text-independent and requires no parallel training data. Some advanced methods transform pitch contours instead, but require either parallel training data or syllabic annotations. We propose a method which retains the simplicity and text-independence of the frame-level conversion while yielding high-quality conversion. We achieve these goals by (1) introducing a text-independent tri-frame alignment method, (2) including delta features of F0 into Gaussian mixture model (GMM) conversion and (3) reducing the well-known GMM oversmoothing effect by F0 histogram equalization. Our objective and subjective experiments on the CMU Arctic corpus indicate improvements over both the mean/variance normalization and the baseline GMM conversion.},
  language = {en},
  author = {Wu, Zhi-Zheng and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
  year = {2010},
  pages = {4},
  file = {/Users/kennylino/Zotero/storage/AKVLGYDG/Wu et al. - Text-Independent F0 Transformation with Non-Parall.pdf}
}

@book{zampini2008,
  title = {Phonology and {{Second Language Acquisition}}},
  abstract = {The focus of this series is on psycholinguistic and sociolinguistic aspects of bilingualism. This entails topics such as childhood bilingualism, psychological models of bilingual language users, language contact and bilingualism, maintenance and shift of minority languages, and socio-political aspects of bilingualism.},
  author = {Zampini, Mary L. and Bot, Kees De and Ayoun, Dalila and Clyne, Michael and Davis, Kathryn A. and Fishman, Joshua A. and Grosjean, Francois and Neuch{\^a}tel, Universit{\'e} De and Huebner, Thom and Klein, Wolfgang and Luedi, Georges and Paulston, Christina Bratt and Romaine, Suzanne and Swain, Merrill and Tucker, G. Richard and Edwards, Jette G. Hansen and Zampini, Mary L.},
  year = {2008},
  file = {/Users/kennylino/Documents/Articles/ZampiniM/undefined/Zampini_Second Language Acquisition Edited by.pdf;/Users/kennylino/Zotero/storage/3YY5CUSF/summary.html}
}

@incollection{chun2008,
  title = {Technologies for Prosody in Context: {{Past}} and Future of {{L2}} Research and Practice},
  author = {Chun, Dorothy M. and Hardison, Debra M. and Pennington, Martha C.},
  year = {2008},
  pages = {323-346}
}

@article{hardison2005,
  title = {Contextualized {{Computer}}-Based {{L2 Prosody Training}}: {{Evaluating}} the {{Effects}} of {{Discourse Context}} and {{Video Input}}},
  volume = {22},
  abstract = {Two types of contextualized input in prosody training were investigated for 28 advanced L2 speakers of English (L1 Chinese). Their oral presentations provided training materials. Native-speakers (NSs) of English provided global prosody ratings, and participants completed questionnaires on perceived training effectiveness. Two groups received training input using Anvil, a web-based annotation tool integrating the video of a speech event with visual displays of the pitch contour, and practiced with Real-Time Pitch (RTP) in Computerized Speech Lab including feedback from a NS. Two groups used only RTP to view their pitch contours and practiced with the same feedback. Within these pairs, one group received discourse-level input and the other individual sentences. Each group served as its own control in a time-series design. All had comparable levels of performance prior to training. Results indicated that although all groups improved as a result of training, discourse-level input produced better transfer to novel natural discourse. The presence of video was more helpful with discourselevel input than with individual sentences. Speech samples collected 1 week after training revealed sustained improvement. Questionnaire results support the use of computer-based tools and authentic speech samples. Findings strongly suggest that meaningful contextualized input is valuable in prosody training when the measurement is at the level of extended connected speech typical of natural discourse.},
  language = {en},
  number = {2},
  journal = {CALICO Journal},
  author = {Hardison, Debra M.},
  year = {2005},
  pages = {16},
  file = {/Users/kennylino/Zotero/storage/DEL2DFGZ/HARDISON - Contextualized Computer-based L2 Prosody Training.pdf}
}

@article{strauss2016,
  title = {Results of the {{WNUT16 Named Entity Recognition Shared Task}}},
  abstract = {This paper presents the results of the Twitter Named Entity Recognition shared task associated with W-NUT 2016: a named entity tagging task with 10 teams participating. We outline the shared task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task.},
  language = {en},
  author = {Strauss, Benjamin and Toma, Bethany E and Ritter, Alan},
  year = {2016},
  pages = {7},
  file = {/Users/kennylino/Zotero/storage/UFERALDN/Strauss et al. - Results of the WNUT16 Named Entity Recognition Sha.pdf}
}

@article{limsopatham2016,
  title = {Bidirectional {{LSTM}} for {{Named Entity Recognition}} in {{Twitter Messages}}},
  abstract = {In this paper, we present our approach for named entity recognition in Twitter messages that we used in our participation in the Named Entity Recognition in Twitter shared task at the COLING 2016 Workshop on Noisy User-generated text (WNUT). The main challenge that we aim to tackle in our participation is the short, noisy and colloquial nature of tweets, which makes named entity recognition in Twitter messages a challenging task. In particular, we investigate an approach for dealing with this problem by enabling bidirectional long short-term memory (LSTM) to automatically learn orthographic features without requiring feature engineering. In comparison with other systems participating in the shared task, our system achieved the most effective performance on both the `segmentation and categorisation' and the `segmentation only' sub-tasks.},
  language = {en},
  author = {Limsopatham, Nut and Collier, Nigel},
  year = {2016},
  pages = {8},
  file = {/Users/kennylino/Zotero/storage/84R23D7R/Limsopatham and Collier - Bidirectional LSTM for Named Entity Recognition in.pdf}
}

@article{ritter2011,
  title = {Named {{Entity Recognition}} in {{Tweets}}: {{An Experimental Study}}},
  abstract = {People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F1 by 25\% over ten common entity types.},
  language = {en},
  author = {Ritter, Alan and Clark, Sam},
  year = {2011},
  pages = {11},
  file = {/Users/kennylino/Zotero/storage/HL48XPIK/Ritter and Clark - Named Entity Recognition in Tweets An Experimenta.pdf}
}

@inproceedings{baldwin2015,
  title = {Shared {{Tasks}} of the 2015 {{Workshop}} on {{Noisy User}}-Generated {{Text}}: {{Twitter Lexical Normalization}} and {{Named Entity Recognition}}},
  shorttitle = {Shared {{Tasks}} of the 2015 {{Workshop}} on {{Noisy User}}-Generated {{Text}}},
  doi = {10.18653/v1/W15-4319},
  abstract = {This paper presents the results of the two shared tasks associated with W-NUT 2015: (1) a text normalization task with 10 participants; and (2) a named entity tagging task with 8 participants. We outline the task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task.},
  language = {en},
  publisher = {{Association for Computational Linguistics}},
  author = {Baldwin, Timothy and {de Marneffe}, Marie-Catherine and Han, Bo and Kim, Young-Bum and Ritter, Alan and Xu, Wei},
  year = {2015},
  pages = {126-135},
  file = {/Users/kennylino/Zotero/storage/QM6PV3IT/Baldwin et al. - 2015 - Shared Tasks of the 2015 Workshop on Noisy User-ge.pdf}
}

@inproceedings{pennington2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  doi = {10.3115/v1/D14-1162},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  language = {en},
  publisher = {{Association for Computational Linguistics}},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532-1543},
  file = {/Users/kennylino/Zotero/storage/XQXF3V5D/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}

@inproceedings{godin2015,
  title = {Multimedia {{Lab}} \$@\$ {{ACL WNUT NER Shared Task}}: {{Named Entity Recognition}} for {{Twitter Microposts}} Using {{Distributed Word Representations}}},
  shorttitle = {Multimedia {{Lab}} \$@\$ {{ACL WNUT NER Shared Task}}},
  doi = {10.18653/v1/W15-4322},
  abstract = {Due to the short and noisy nature of Twitter microposts, detecting named entities is often a cumbersome task. As part of the ACL2015 Named Entity Recognition (NER) shared task, we present a semisupervised system that detects 10 types of named entities. To that end, we leverage 400 million Twitter microposts to generate powerful word embeddings as input features and use a neural network to execute the classification. To further boost the performance, we employ dropout to train the network and leaky Rectified Linear Units (ReLUs). Our system achieved the fourth position in the final ranking, without using any kind of hand-crafted features such as lexical features or gazetteers.},
  language = {en},
  publisher = {{Association for Computational Linguistics}},
  author = {Godin, Fr{\'e}deric and Vandersmissen, Baptist and De Neve, Wesley and {Van de Walle}, Rik},
  year = {2015},
  pages = {146-153},
  file = {/Users/kennylino/Zotero/storage/FLQ3JJWZ/Godin et al. - 2015 - Multimedia Lab $@$ ACL WNUT NER Shared Task Named.pdf}
}

@inproceedings{plank2014,
  title = {Learning Part-of-Speech Taggers with Inter-Annotator Agreement Loss},
  doi = {10.3115/v1/E14-1078},
  abstract = {In natural language processing (NLP) annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations. However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and interannotator agreement is often less than perfect. While annotation projects usually specify how to deal with linguistically debatable phenomena, annotator disagreements typically still stem from these ``hard'' cases. This indicates that some errors are more debatable than others. In this paper, we use small samples of doublyannotated part-of-speech (POS) data for Twitter to estimate annotation reliability and show how those metrics of likely interannotator agreement can be implemented in the loss functions of POS taggers. We find that these cost-sensitive algorithms perform better across annotation projects and, more surprisingly, even on data annotated according to the same guidelines. Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking.},
  language = {en},
  publisher = {{Association for Computational Linguistics}},
  author = {Plank, Barbara and Hovy, Dirk and S{\o}gaard, Anders},
  year = {2014},
  pages = {742-751},
  file = {/Users/kennylino/Zotero/storage/Y5UIMKYB/Plank et al. - 2014 - Learning part-of-speech taggers with inter-annotat.pdf}
}


